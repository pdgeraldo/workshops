---
title: "Potential outcomes"
author: "Pablo Geraldo Bast√≠as"
logo: "https://fundit.fr/sites/default/files/styles/max_650x650/public/institutions/capture-decran-2023-07-07-162216.png?itok=1CkwlJEu"
include-in-header:
  - text: |
      <style>
      .reveal .slide-logo {
        max-height: unset;
        height: 100px;
      }
      </style>
footer: "Demystifying Causal Inference - Potential Outcomes"
date: 05/28/2024
date-format: long
format: 
  revealjs:
    theme: simple
    width: 1600
    height: 900
    transition: slide
    slide-number: c/t
    chalkboard: true
    auto-stretch: false
callout-appearance: simple
---

# Outline {background-color="#17a091"}

\newcommand\indep{\perp\!\!\!\perp}
\newcommand\nindep{\not\!\perp\!\!\!\perp}

## Lecture 1: From causal methods to causal assumptions

It is common to hear the idea that some *methods* are inherently causal: propensity score matching, IPW, instrumental variables, panel methods, and even machine learning

But there is nothing properly causal about a method (i.e, an estimator, an algorithm, a formula)!

What makes our conclusions causal is the assumptions we make, whose plausibility is justified on a given research design

. . .

In this first lecture, we are going to study the formal language that allows us to clearly and transparently state both our quantities of interest and our assumptions

Then we will see this framework in action, to understand how does it work in the context on randomised studies

We should be able to see why RCTs are often described as the "gold standard" for causal inference

## The First Commandment of Causal Inference {background-color="#692044"}

:::{.fragment}

### (by Chad Hazlett)

::: callout-warning
Thou shalt not blindly apply identification strategies as if the *procedure* makes your results causal
:::

:::
. . .

Identification strategies are about assumptions, not tools:

-   learning tools is important, useful, and fun

-   but the tool you use doesn't make your result causal, your assumptions do

-   most of your effort should go into understanding and communicating whether your assumptions are credible

. . .

::: callout-note
## Corolary: Never say "my estimate is causal because I used (.)"

Where (.) = matching, IPW, IV, whatever
:::


# Potential Outcomes {background-color="#17a091"}

## Neyman, Neyman-Rubin, or Rubin Causal Model?

<br>

Introduced by Neyman (1923) in the context of experimental design. They remained only used in that context for decades!

-   Imported and developed by Donald Rubin for observational studies (c. 1974)

-   They are really great to clarify *what do we want to know* ([[**estimand**]()]{.fragment .fade-in})

-   This includes identifying *reasons for discrepancies* between what we observe and our estimand ([[**bias**]()]{.fragment .fade-in})

-   They are great to formalize *what needs to be true* for our estimand to be identified with a given *estimator* ([[**assumptions**]()]{.fragment .fade-in})

. . .

-   They are not-so-great to assess if our assumptions are plausible or defensible (more on this soon!)

------------------------------------------------------------------------

## Notation

<br>

Let's start with some definitions:

. . .

$Y$ is the outcome variable *as we observe it*

$D$ is the variable whose effect we want to study (treatment, exposure)

$Y_d$ is the potential outcome when we [**set**]{.fragment .highlight-red} $D=d$. For example, when $D \in \{0,1\}$:

-   $Y_1$ is the potential outcome under "treatment"

-   $Y_0$ is the potential outcome under "control"

------------------------------------------------------------------------

## Notation

You will find a lot of equivalent notations for potential outcomes. It could be confusing, but it is good to get practice working with different variants

$$Y(d) = Y_d = Y^d$$

. . .

::: callout-tip
## Read it like this:

The value that the variable $Y$ would take if we were to set/manipulate the variable $D$ to the value $d$.
:::

::: {.fragment .r-stack}
*Can you ever observe any of those potential outcomes?*
:::

. . .

**Consistency** (also known as SUTVA): $$D = d \rightarrow Y = Y_d$$

For the binary treatment case, we have: $Y = DY_1 + (1-D)Y_0$ ("switching equation")

. . .

::: callout-note
What are the assumptions built in this notation? What type of dependence are we ruling out?
:::

------------------------------------------------------------------------

## Checking intuitions

. . .

* Is $E[Y_{1i} | D=_1] = E[Y_i | D_i=1]$?

. . .

* If we throw $D_i$ at random in this room:

  + How would the observed $Y_{1i}$ (for those with $D_i=1$) compare to the $Y_{1i}$ you don't see?
  
  + What about the $Y_{0i}$?
  
  + Can we estimate $E[Y_1]$ and $E[Y_0]$?
  
. . .

* Suppose $Y_{1i} = Y_{0i}$ for everyone, and I can see them all:

  + What happens if I give $D_i=1$ more often to those with higher $Y_1$?
  
  + How would the $Y_1$ you see vs the $Y_1$ you don't see relate?
  
  + What would $E[Y_i|D_i=1] - E[Y_i | D_i=0]$ tell me?

## Causal estimands

The first thing that potential outcomes allow us to do is to formalize the causal effects we may want to estimate.

Some frequently invoked estimands are the following:

[$$ITE = \tau_i = Y_{1i} - Y_{0i}$$]{.fragment .fade-in-then-out}

[$$
ATE = E[\tau_i] = E[Y_{1i} - Y_{0i}] = E[Y_{1i}] - E[Y_{0i}]
$$]{.fragment .fade-in-then-out}

[$$
ATT = E[\tau_i|D_i=1] = E[Y_{1i} - Y_{0i}|D_i=1] = E[Y_{1i}|D_i=1] - E[Y_{0i}|D_i=1]
$$]{.fragment .fade-in-then-out}

[$$
ATC = E[\tau_i|D_i=0] = E[Y_{1i} - Y_{0i}|D_i=0] = E[Y_{1i}|D_i=0] - E[Y_{0i}|D_i=0]
$$]{.fragment .fade-in-then-out}

[$$
CATE = E[\tau_i|X_i=x] = E[Y_{1i} - Y_{0i}|X_i=x] = E[Y_{1i}|X_i=x] - E[Y_{0i}|X_i=x]
$$]{.fragment .fade-in-then-out}

------------------------------------------------------------------------

## What is your estimand?

Researchers, at least social scientists, tend to formalize their effect of interest as **regression coefficients** (i.e., their hypothesis are formulated within a statistical model)

Potential outcomes offer a way to formalize what we mean by a *causal effect* outside any statistical model. This allows us to clearly separate *what do we want* (a certain **estimand**), the statistical machinery to answer our question (an **estimator**) and the particular answer we get (our empirical **estimate**).

Lundberg, Johnson, and Stewart (2021) discuss this point in great detail. Absolutely worth reading! Ungated version [here](https://osf.io/preprints/socarxiv/ba67n/).


## What is your estimand?



![](img/Lundberg_estimand.jpeg){fig-align="center" width=70%}


## Short activity (3 mins) {background-color="#692044"}


Take a moment to think about your own research:

-   What **causal** question is relevant for you to study?
-   Can you formulate it using potential outcomes?
  + What is the contrast of interest?
  + Averaging over which sub-population?
-   What are the assumptions your are making in this formalization?

------------------------------------------------------------------------

## The Science Table

```{r, echo=FALSE}
data <- 
  data.frame(D=c(rep("A",5),rep("B",5)),
         Ya=c(1,0,0,1,1,1,1,0,0,1),
         Yb=c(1,0,1,0,1,0,0,0,1,1),
         W=c("quant","quant","qual","quant","qual",
             "qual","quant","qual","qual","quant")) |> 
  dplyr::mutate(Y = ifelse(D=="A",Ya,Yb))

ATE <- mean(data$Ya-data$Yb)
ATE_qual <- mean(data$Ya[data$W=="qual"]) -
  mean(data$Yb[data$W=="qual"])
ATE_quant <- mean(data$Ya[data$W=="quant"]) -
  mean(data$Yb[data$W=="quant"])
DIM <- mean(data$Y[data$D=="A"]) - mean(data$Y[data$D=="B"])
DIM_quant <- mean(data$Y[data$D=="A" & data$W=="quant"]) - 
  mean(data$Y[data$D=="B" & data$W=="quant"])
DIM_qual <- mean(data$Y[data$D=="A" & data$W=="qual"]) - 
  mean(data$Y[data$D=="B" & data$W=="qual"])
weights_quant <- sum(data$W=="quant")/nrow(data)
weights_qual <- sum(data$W=="qual")/nrow(data)
DIM_w <- DIM_quant*weights_quant + DIM_qual*weights_qual
```

::: columns
::: {.column width="50%"}
One advantage of PO is that we can treat them directly as random variables!

So, everything we already know related to probability manipulation still applies here.

The basic calculation device (usually implicit) for that matter is the **science table**.

Basically, the full schedule response of the potential outcomes under different treatment conditions
:::

::: {.column width="50%"}
| Unit | $D_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ |
|------|-------|-------|----------|----------|----------|
| 1    | A     | 1     | 1        | 1        | 0        |
| 2    | A     | 0     | 0        | 0        | 0        |
| 3    | A     | 0     | 0        | 1        | -1       |
| 4    | A     | 1     | 1        | 0        | 1        |
| 5    | A     | 1     | 1        | 1        | 0        |
| 6    | B     | 0     | 1        | 0        | 1        |
| 7    | B     | 0     | 1        | 0        | 1        |
| 8    | B     | 0     | 0        | 0        | 0        |
| 9    | B     | 1     | 0        | 1        | -1       |
| 10   | B     | 1     | 1        | 1        | 0        |
:::
:::

## Average Treatment Effect

::: columns
::: {.column width="50%"}
Imagine we want to compare two educational programs, $a$ and $b$.

We are interested in the employment status of their graduates after a year ($Y$)

The causal effect of the program would be the comparison of the potential outcomes $Y_{ai}$ and $Y_{bi}$, for all units $i$

$$\text{ATE} = E(Y_{ai}) - E(Y_{bi})$$ $$(6/10) - (5/10) = \mathbf{\color{blue}{0.1}}$$
:::

::: {.column width="50%"}
| Unit | $D_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ |
|------|-------|-------|----------|----------|----------|
| 1    | A     | 1     | 1        | 1        | 0        |
| 2    | A     | 0     | 0        | 0        | 0        |
| 3    | A     | 0     | 0        | 1        | -1       |
| 4    | A     | 1     | 1        | 0        | 1        |
| 5    | A     | 1     | 1        | 1        | 0        |
| 6    | B     | 0     | 1        | 0        | 1        |
| 7    | B     | 0     | 1        | 0        | 1        |
| 8    | B     | 0     | 0        | 0        | 0        |
| 9    | B     | 1     | 0        | 1        | -1       |
| 10   | B     | 1     | 1        | 1        | 0        |
:::
:::

------------------------------------------------------------------------

## Difference-in-means

::: columns
::: {.column width="50%"}
But we don't observe all potential outcomes for all units!

Can we use instead the observational comparison as a proxy?

Let's calculate the different in means across groups, based on the *actual* program each subject attended:

$$\text{diff-in-means} = E(Y_i|X=a) - E(Y_i|X=b)$$ $$(3/5)-(2/5) = \mathbf{\color{red}{0.2}}$$

$$\color{red}{\text{diff-in-means} \neq \text{ATE}}$$

$\color{red}{\text{But why???}}$
:::

::: {.column width="50%"}
| Unit | $D_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ |
|------|-------|-------|----------|----------|----------|
| 1    | A     | 1     | 1        | .        | .        |
| 2    | A     | 0     | 0        | .        | .        |
| 3    | A     | 0     | 0        | .        | .        |
| 4    | A     | 1     | 1        | .        | .        |
| 5    | A     | 1     | 1        | .        | .        |
| 6    | B     | 0     | .        | 0        | .        |
| 7    | B     | 0     | .        | 0        | .        |
| 8    | B     | 0     | .        | 0        | .        |
| 9    | B     | 1     | .        | 1        | .        |
| 10   | B     | 1     | .        | 1        | .        |
:::
:::

------------------------------------------------------------------------

## Sources of bias

::: columns
::: {.column width="50%"}
$$E(\text{diff-in-means})$$ $$= E(Y_i|X=a) - E(Y_i|X=b)$$ $$= E(Y_a|X=a) - E(Y_b|X=b)$$ $$= ATE +$$ $$(E[Y_b|X=a] - E[Y_b|X=b])+$$ $$(1-P[X])(ATT-ATC)$$ $$= \mathbf{\color{blue}{0.1}} + \color{red}{0.2 + (0.5)(-0.2) = \mathbf{0.2}}$$
:::

::: {.column width="50%"}
| Unit | $D_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ |
|------|-------|-------|----------|----------|----------|
| 1    | A     | 1     | 1        | .        | .        |
| 2    | A     | 0     | 0        | .        | .        |
| 3    | A     | 0     | 0        | .        | .        |
| 4    | A     | 1     | 1        | .        | .        |
| 5    | A     | 1     | 1        | .        | .        |
| 6    | B     | 0     | .        | 0        | .        |
| 7    | B     | 0     | .        | 0        | .        |
| 8    | B     | 0     | .        | 0        | .        |
| 9    | B     | 1     | .        | 1        | .        |
| 10   | B     | 1     | .        | 1        | .        |
:::
:::

------------------------------------------------------------------------

## Identification assumptions

::: columns
::: {.column width="50%"}
We need the following condition to be true: $$
Y_d \perp\!\!\!\perp D 
$$

Do we meet that condition here? No! $$(Y_{ai},Y_{bi}) \not\!\perp\!\!\!\perp D$$

Because: $$P(Y_a = y | D=a) \neq P(Y_a = y)$$ $$P(Y_b = y | D=b) \neq P(Y_b = y)$$
:::

::: {.column width="50%"}
| Unit | $D_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ |
|------|-------|-------|----------|----------|----------|
| 1    | A     | 1     | 1        | 1        | 0        |
| 2    | A     | 0     | 0        | 0        | 0        |
| 3    | A     | 0     | 0        | 1        | -1       |
| 4    | A     | 1     | 1        | 0        | 1        |
| 5    | A     | 1     | 1        | 1        | 0        |
| 6    | B     | 0     | 1        | 0        | 1        |
| 7    | B     | 0     | 1        | 0        | 1        |
| 8    | B     | 0     | 0        | 0        | 0        |
| 9    | B     | 1     | 0        | 1        | -1       |
| 10   | B     | 1     | 1        | 1        | 0        |
:::
:::

------------------------------------------------------------------------

## Identification assumptions

::: columns
::: {.column width="50%"}
What about including another covariate $W$?

Does the following condition holds? $$
Y_d \perp\!\!\!\perp D | W
$$

Not quite either! But still "better" than before, right? right?

Let's define: $$\text{CATE}_{w} = E(Y_a-Y_b|W=w)$$

and the estimator $\widehat{\text{CATE}}_{w} =$ $$E(Y_i|X=a,W=w) - E(Y_i|X=b,W=w)$$
:::

::: {.column width="50%"}
| Unit | $D_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ | $W_i$ |
|------|-------|-------|----------|----------|----------|-------|
| 1    | A     | 1     | 1        | 1        | 0        | Quant |
| 2    | A     | 0     | 0        | 0        | 0        | Quant |
| 3    | A     | 0     | 0        | 1        | -1       | Qual  |
| 4    | A     | 1     | 1        | 0        | 1        | Quant |
| 5    | A     | 1     | 1        | 1        | 0        | Qual  |
| 6    | B     | 0     | 1        | 0        | 1        | Qual  |
| 7    | B     | 0     | 1        | 0        | 1        | Quant |
| 8    | B     | 0     | 0        | 0        | 0        | Qual  |
| 9    | B     | 1     | 0        | 1        | -1       | Qual  |
| 10   | B     | 1     | 1        | 1        | 0        | Quant |
:::
:::

------------------------------------------------------------------------

## Identification assumptions

::: columns
::: {.column width="50%"}
$$\widehat{\text{CATE}}_{Quant} = \widehat{\text{CATE}}_{Qual} = 0.16$$ $$\text{ATE} = \sum_w\text{CATE}_w P(w)$$ $$\widehat{\text{ATE}} = (0.5)(0.16) + (0.5)(0.16) = 0.16$$

However, look a the true $\text{ATE}_W$: $$\text{CATE}_{Quant} = -0.2$$ $$\text{CATE}_{Qual} = 0.4$$ $$\text{ATE} = (0.5)(-0.2) + (0.5)(0.4) = \color{blue}{\mathbf{0.1}}$$
:::

::: {.column width="50%"}
| Unit | $D_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ | $W_i$ |
|------|-------|-------|----------|----------|----------|-------|
| 1    | A     | 1     | 1        | 1        | 0        | Quant |
| 2    | A     | 0     | 0        | 0        | 0        | Quant |
| 3    | A     | 0     | 0        | 1        | -1       | Qual  |
| 4    | A     | 1     | 1        | 0        | 1        | Quant |
| 5    | A     | 1     | 1        | 1        | 0        | Qual  |
| 6    | B     | 0     | 1        | 0        | 1        | Qual  |
| 7    | B     | 0     | 1        | 0        | 1        | Quant |
| 8    | B     | 0     | 0        | 0        | 0        | Qual  |
| 9    | B     | 1     | 0        | 1        | -1       | Qual  |
| 10   | B     | 1     | 1        | 1        | 0        | Quant |
:::
:::

------------------------------------------------------------------------

## So, how do we know?

::: columns
::: {.column width="50%"}
In general, we rely on **extra-statistical assumptions** about the data generating process to claim causal identification.

::: callout-tip
## "No causes in, no cases out"

Nancy Cartwright
:::

Is there a way to design an study in which we know, **by design**, that the needed assumptions hold?
:::

::: {.column width="50%"}
| Unit | $D_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ | $W_i$ |
|------|-------|-------|----------|----------|----------|-------|
| 1    | A     | 1     | 1        | .        | .        | Quant |
| 2    | A     | 0     | 0        | .        | .        | Quant |
| 3    | A     | 0     | 0        | .        | .        | Qual  |
| 4    | A     | 1     | 1        | .        | .        | Quant |
| 5    | A     | 1     | 1        | .        | .        | Qual  |
| 6    | B     | 0     | .        | 0        | .        | Qual  |
| 7    | B     | 0     | .        | 0        | .        | Quant |
| 8    | B     | 0     | .        | 0        | .        | Qual  |
| 9    | B     | 1     | .        | 1        | .        | Qual  |
| 10   | B     | 1     | .        | 1        | .        | Quant |
:::
:::

# Randomised Experiments {background-color="#17a091"}

## Why randomisation?

If we want to **predict under interventions**, then the best way to do it is **interveening**!

Random assignment (more specifically, RCTs) has been called the **gold standard** for causal inference: it guarantees the necessary assumptions for causal inference hold by design.

When unfeasible, imagining a hypothetical experiment still offers a useful benchmark to assess the validity of causal claims, and even to clarify what do we mean by a particular causal effect.

Experiments come in many different flavors: lab, field, survey, and even quasi-experiments!

Here we will only scratch the surface of social science experiments: the idea is to get you interested and point you to the resources out there

------------------------------------------------------------------------


## Why randomisation? Intuition

Let's go back for a second to the switching equation introduced earlier: 

. . .

$$
Y = DY_1 + (1-D)Y_0
$$

. . .

Basically, the idea (somewhat counter-intuitive) is that:

* Potential outcomes are fixed ($Y_1$ and $Y_0$ just exists out there)

* The treatment's $D$ sole function is to "reveal" one or another!

Then, it is clear that, when $D$ is randomly assigned, we get a random sample of both $Y_1$ and $Y_0$, allowing us to estimate the ATE

:::aside
We won't discuss it here, but there are ways to conceptualize potential outcomes as random instead of fixed. This mostly affects estimation rather than identification
:::


## Why randomisation?

### (more formally)

We already saw that we can identify the causal effect of $D$ on $Y$ if the treatment assignment is independent of the potential outcomes. Formally $$(Y_d,Y_{d^*}) \perp\!\!\!\perp D$$

Recall the diff-in-means decomposition we review earlier. Given the **ignorability** of the treatment assignment, we have can further write it as:

By consistency

$$E(Y_i|D=d) - E(Y_i|D=d^*) = E(Y_d|D=d) - E(Y_{d^*}|D=d^*)$$

## Why randomisation?

### (more formally)

With some algebra

$$= E(Y_d - Y_{d^*}) + (E[Y_{d^*}|D=d] - E[Y_{d^*}|D=d^*])+(1-P[D])(ATT-ATC)$$

And, by ignorability, this simplifies to

$$E(Y_d - Y_{d^*}) = ATE$$

------------------------------------------------------------------------

## Forms of validity

Traditionally, researchers argue about the validity of a study's causal conclusion (and, more generally, about the validity of different research designs) based on the potential biases that pose **threats to validity**. Check [this](https://journals.lww.com/epidem/Fulltext/2020/05000/A_Graphical_Catalog_of_Threats_to_Validity_.11.aspx) amazing paper by Matthay and Glymour for a review.

We reviewed the bias in the difference-in-means estimator: baseline differences (under the control condition), and differential response to the treatment (under the exposure condition).

. . .

But when we randomize an exposure, we know that who ends up in each treatment arm has nothing to do with their potential outcomes!

. . .

This is why we generally say that experiments are great for **internal validity**: among the people that participated in our study, we can rule out systematic sources of bias.

However, this does not imply that our results are **externally valid**, i.e., that they apply to people outside our study! We need further assumptions to move from one to another.

## Types of experiments

-   **Laboratory experiments**: Usually conducted with a small sample (of undergraduate psychology students), many times involving games in a computer. Helpful for cognitive/behavioral questions.

-   **Field experiments**: In order to obtain more *externally valid* results, experiments conducted in the field (i.e., under real-world conditions) are the way to go. Definitely more expensive though. Audit studies are a particular type of field experiment.

-   **Survey experiments**: One can randomize treatment conditions *in a survey* to evaluate how participants change their responses based on certain stimulus. Vignettes and list experiments are examples of this approach.

-   **(Bonus) Quasi-experiments**: Researchers usually call quasi-experiments to real-world situations that offer as-if random variation in a treatment of interest. For example, earthquakes, change in laws, date of birth, etc.

------------------------------------------------------------------------


## Short activity (3 mins) {background-color="#692044"}

Sometimes it is hard to imagine an experiment that would be relevant for the type of questions we care about.

Some people even say (and I for sure partially agree!) that experiments tend to emphasize "small" versus "big" questions, promoting incremental/testable policies.

However, there are tons of examples of researchers using experiments to address important, **big** and difficult questions. **Do you know of any example?**

Take a moment to check the syllabus of UCLA professor Graeme Blair's Experimental Design class [here](https://graemeblair.com/teaching/UCLA_PS200E_Syllabus.pdf). He put together a list of experiments conducted by UCLA faculty, and by graduate students.

------------------------------------------------------------------------

## How to randomise?

We use randomisation not just for identification (**ignorability**), but also for estimation!

If we assume the potential outcomes are fixed, and the only thing that varies is the treatment assignment scheme, we can derive a **permutation distribution** and use it for inference.

How much dispersion (i.e. uncertainty) is in our distribution will be affected by **the level** at which randomization (or, more precisely, the treatment) happens: is it at the individual level? or at a cluster/group level?

::: callout-note
The more the aggregation, the more uncertainty. So why would we want to randomize at the cluster level?
:::

Conditional randomization (i.e., blocking) increase efficiency, when we have variables that are highly predictive of the outcome of interest

::: callout-note
One extreme of this is randomization in matched pairs: for each pair of individual with similar covariates, we randomly assign one to treatment and one to control
:::

## Blocking

Similar to the intuition for stratified random *sampling* in the context of surveys, blocking may increase precision in experimental design

Precision gains are similar to increasing the sample size (more on this later)

* Collect background information on covariates relevant *to the outcome*

* Pre-stratify your sample, then randomise within blocks

  + This ensures that, with respect to the blocked factors, both treatment arms are identical
  
  + It is essentially the same as running a separate experiment in each strata
  
  
## Blocking 

For estimation, obtain block-specific effects, and average according to population shares. With $J$ strata:

$$
\tau_{\text{block}} = \sum_{j=1}^{J} \frac{N_j}{N} \tau_j
$$
with variance: 

$$V(\tau_{\text{block}}) = \sum_{j=1}^{J} \Big(\frac{N_j}{N}\Big)^2 V(\tau_j)$$

:::aside
When would this be equal to regression with block fixed effects?
:::

## How to estimate?

Once we have random assignment, estimation is super easy!

All that matters is that your estimation procedure follows your (known) assignment rule

:::{.callout-warning}
## In other words: Estimate as you randomise!
:::

. . . 

* A simple difference in means will recover the ATE

* This is equivalent to running a bivariate regression (no controls)

* With controls is a little trickier (see [Freedman 2008](https://projecteuclid.org/journals/annals-of-applied-statistics/volume-2/issue-1/On-regression-adjustments-in-experiments-withseveral-treatments/10.1214/07-AOAS143.full) and [Lin 2013](https://projecteuclid.org/journals/annals-of-applied-statistics/volume-7/issue-1/Agnostic-notes-on-regression-adjustments-to-experimental-data--Reexamining/10.1214/12-AOAS583.full)), but more useful (increased precision)!

* Your standard errors depend on your randomisation scheme! 
  + is assignment at the individual level
  + do probabilities vary by block
  + is randomisation conducted among clusters?

## Covariate balance

When you randomise the treatment, pre-treatment variables will be (in expectation) balanced across treatment arms

This is usually a way to "check" if the randomisation step worked out correctly

However, one should be careful to not discard "unlucky" realizations in an arbitrary manner

Also, note that imbalance **does not** imply bias: properly estimated standard errors will account for uncertainty in the treatment effect!

* Check this great series by Senn: [blog](https://errorstatistics.com/2020/04/20/s-senn-randomisation-is-not-about-balance-nor-about-homogeneity-but-about-randomness-guest-post/), [slides](https://www.ideal.rwth-aachen.de/wp-content/uploads/2014/02/Senn_Randomisation.pdf), [article](https://onlinelibrary.wiley.com/doi/full/10.1002/sim.5713)


## Potential issues

In random-land, everything is perfect!

However, many things can go wrong in practice, threatenind *internal validity*:

* **Randomisation failure**: for some reason, the physical procedure of assignment fails to be random (see Vietnam draft lottery)

* **Noncompliance**: many times, participants fail to adhere to their assigned treatment status (one- or two-sided noncompliance)

* **Attrition and lost to follow up**: even if randomisation is properly conducted and everyone adheres to the treatment, we may differentially lost participants in the post-treatment period!

## Potential issues

![](https://imgs.xkcd.com/comics/research_ethics.png){.fragment width="600" align}

:::aside
[ethics](https://imgs.xkcd.com/comics/research_ethics.png)
:::


------------------------------------------------------------------------

## Additional Resources

### Online learning

-   A selected and annotated bibliography on causality [here](https://www.pablogeraldo.com/pdf/GeraldoBrand_2020_Causality.pdf)

-   J-PAL research resources [here](https://www.povertyactionlab.org/research-resources?view=toc)

-   EGAP methods guides [here](https://egap.org/methods-guides/)

### Textbooks

-   Gerber and Green (2012) Field Experiments: Design, Analysis, and Interpretation (check [here](https://www.amazon.com/Field-Experiments-Design-Analysis-Interpretation/dp/0393979954/ref=sr_1_11?dchild=1&keywords=gerber+green&qid=1624466782&sr=8-11))

------------------------------------------------------------------------

## Short activity {background-color="#692044"}

Think in a research question that you could possibly address using an experimental design:

-   What is your research question?

-   What is your **estimand**? (effect of what? on what? among whom?)

-   What type of experiment would you conduct? (lab? field? survey?)

-   What would be the level of your randomisation? (individual? cluster? why?)

- What ethical considerations you should take into account for your experiment to be acceptable?
