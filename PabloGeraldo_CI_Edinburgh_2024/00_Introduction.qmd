---
title: "Demystifying Causal Inference"
author: "Pablo Geraldo Bast√≠as"
logo: "https://fundit.fr/sites/default/files/styles/max_650x650/public/institutions/capture-decran-2023-07-07-162216.png?itok=1CkwlJEu"
include-in-header:
  - text: |
      <style>
      .reveal .slide-logo {
        max-height: unset;
        height: 100px;
      }
      </style>
footer: "Demystifying Causal Inference"
date: 05/28/2024
date-format: long
format: 
  revealjs:
    #theme: simple
    width: 1600
    height: 900
    transition: slide
    slide-number: c/t
    chalkboard: true
callout-appearance: minimal
---

# Motivation {background-color="aquamarine"}


## Why should we study causal inference?

. . .

<br>
The social sciences are experimenting what some authors have called a "credibility revolution" (Angrist and Pischke, 2010), an "identification revolution" (Morgan, 2016), or simply a "causal revolution" (Pearl and MacKenzie, 2018)

. . .

In artificial intelligence/ML, causality have been deemed [**"the next frontier"**](https://phys.org/news/2019-02-causal-disentanglement-frontier-ai.html) and [**"the next most important thing"**](https://www.datasciencecentral.com/profiles/blogs/causality-the-next-most-important-thing-in-ai-ml)

. . .

The enormous progress in the last decades has been facilitated by the development of mathematical frameworks that provide researchers with tools to handle causal questions: [Potential Outcomes]{.fragment .highlight-red} and the [Structural Causal Model]{.fragment .highlight-red}

---


## What should you expect from this workshop?

This workshop is designed as a "Crash Course", so we can obviously focus only on a few things:

<br>

* Familiarize yourself with the most widely used CI frameworks

* Understand the role of randomization to tackle causal questions

* Use potential outcomes and the do-operator to formalize causal estimands

* Use directed acyclic graphs (DAGs) to encode qualitative assumptions

* Derive identification results and testable implications from a DAG 

* Assess the plausibility of different identification strategies applied to real problems

<br>

At the end of our two morning sessions, I hope you feel better equipped to read and evaluate the applied literature and to design your own studies using appropriate identification strategies (we are going to discuss two popular strategies this afternoon!).

---


## What are we not covering today?

There is a lot of stuff out there! Some areas that you might be interested in but we don't have enough time to review today:

<br>

* Sequential estimation for time-varying treatments 

* Estimation in general, including Targeted Maximum Likelihood

* Machine learning for heterogeneous treatment effects

* Do-calculus and PO-calculus for identification

* Causal mediation analysis, causal attribution

* Other graphical models (MCM, SWIGs, the Hypothetical Model)

* Selection diagrams for missing data

* Data fusion (generalization, external validity)

* Causal discovery (going from the data to the DAG)

---


## Intuitions about causality
### Have you heard any of these before?

<br>

. . .

"Correlation does not imply causation"


:::{.fragment .r-stack}
*But can we go from one to the other?*
:::


"No causation without manipulation"


:::{.fragment .r-stack}
*Then what about race or gender?*
:::


"Causal inference is a missing data problem"


:::{.fragment .r-stack}
*Or is it the other way around?*
:::


"For causal inference, design trumps analysis"


:::{.fragment .r-stack}
*But what do we mean by design? And analysis?*
:::



# What is causal inference about? {background-color="aquamarine"}

## Statistics/ML vs Causal Inference

::: {.columns}

::: {.column width="50%"}
***Statistics/ML***
  
+ Passive observation of the data generating process
+ Estimand: Joint probabilities, CEF
 $$P(Y,X)$$
 $$E(Y|X=x)$$
+ Focus on asymptotics / out of sample prediction
+ Estimation problem: variance-bias tradeoff
+ Pearl: "deep learning is just curve fitting"
:::

::: {.column width="50%"}
***Causal Inference***

+ Prediction **under interventions** on the DGP
+ Estimand: interventional quantities
$$P(Y|do(x))$$ 
$$E(Y|do(x)) - E(Y|do(x'))$$ 
$$= E(Y_x) - E(Y_{x'})$$
+ Identification problem: consistency (infinite sample)
+ Estimation problem: in general, focus on bias over variance (but changing)
:::

:::
---


## The ladder of causality 

:::{.panel-tabset}

## Association

| Estimand | Activity | Field/Discipline | Questions | Example |
| ---- | --- | --- | ----- | ------ |
| $\mathbf{P(Y \vert X)}$ | Seeing, Observing | Stats, Machine Learning | *What would I believe about Y if I see X?* | What is the expected income of a college graduate in a given field? |

## Interventions
| Estimand | Activity | Field/Discipline | Questions | Example |
| --- | --- | --- | ----- | ------ |
| $\mathbf{P(Y \vert do(x))}$ | Doing, Intervening | Experiments, Policy evaluation | *What would happen with Y if I change X?* | How would income levels change in response to college expansion? |

## Counterfactuals

| Estimand | Activity | Field/Discipline | Questions | Example |
| --- | --- | --- | ----- | ------ |
| $\mathbf{P(Y_x \vert x',y')}$ | Imagining, Retrospecting | Structural Models | *What would have happened with Y have I done X instead of X'? Why?* | What would have been my parents' income have they graduated from college, given that they didn't attend?| 
:::

::: aside
Pearl and Mackenzie (2018)
:::

---

## But, is **doing** *really* that different from **seeing**?


![](https://imgs.xkcd.com/comics/correlation.png)

---

![](https://www.explainxkcd.com/wiki/images/9/9b/selection_bias.png)

![](https://imgs.xkcd.com/comics/cell_phones.png)

![](https://imgs.xkcd.com/comics/confounding_variables.png)

---


## Is **doing** *really* that different from **seeing**?

Let's imagine an example: we want to know the effect of the selectivity of the graduate program a student attends on the quality of their first job after graduation

. . .

::: {.columns}

::: {.column width="40%"}
```{r, echo=TRUE}
set.seed(1988)

# sample size
N <- 1000

# student "potential"
W <- rnorm(N, mean=25, sd=50) 

# program selectivity
X <- 0.6*W + rnorm(N, mean=0, sd=15)

# quality of first job
Y <- 0.4*W - 0.2*X + rnorm(N, mean = 0, sd = 25)

# store everything
data <- data.frame(Y=Y, X=X, W=W)
```
:::

:::{.column width="60%"}
```{r, echo=FALSE, out.width='120%', cache=FALSE}
library(ggplot2)
ggplot2::ggplot(data, aes(x=X, y=Y)) +
  geom_point(alpha=0.2) +
  labs(x = "Selectivity of graduate program index",
       y = "Quality of first job index") +
  theme_bw()
```
:::

:::
---


## Is **doing** *really* that different from **seeing**?

Then, let's see what story a simple linear regression would tell

. . . 

:::{.columns}

:::{.column width="40%"}

```{r, cache = FALSE}
lm(Y~X, data=data)
```

:::{.callout-warning}
However, we know that the true effect is negative!
:::

:::

:::{.column width="60%"}
```{r, echo=FALSE, out.width='120%', cache= FALSE}
ggplot2::ggplot(data, aes(x=X, y=Y)) +
  geom_point(alpha=0.2) +
  geom_smooth(method="lm", color="red") +
  labs(x = "Selectivity of graduate program index",
       y = "Quality of first job index") +
  theme_bw()
```
:::

:::
---



## Is **doing** *really* that different from **seeing**?


What if we instead randomize students to different programs?

:::{.columns}

:::{.column width="40%"}

```{r, cache = FALSE, echo=TRUE}
# let's simulate random assignment 
# of students to programs!
X <- sample(min(X):max(X),
            N, replace=TRUE)
# quality of first job
Y <- 0.3*W - 0.2*X + rnorm(N)
data$Xrand <- X
data$Ytrue <- Y

lm(Ytrue~Xrand, data=data)
```
:::

:::{.column widht="60%"}
```{r, echo=FALSE, out.width='120%', cache = FALSE}
ggplot2::ggplot(data, aes(x=Xrand, y=Ytrue)) +
  geom_point(alpha=0.2) +
  geom_smooth(method="lm", color="red") +
  labs(x = "Selectivity of graduate program index",
       y = "Quality of first job index") +
  theme_bw()
```

:::{.callout-note}
Now we can see the true effect!
:::

:::

:::


---


## Is **doing** *really* that different from **seeing**?

Can we fix the observational comparison by adjusting for $W$?

. . . 

:::{.columns}

:::{.column width="40%"}
```{r, cache = FALSE}
# Same regression
# Plus controls
reg <- lm(Y~X+W, data=data)
```
:::

:::{.column width="60%"}
```{r, echo=FALSE, out.width='120%', cache =FALSE}
ggplot2::ggplot(data, 
                aes(x=X,
                    y=predict(reg, type = "response"))) +
  geom_point(alpha=0.2) +
  geom_smooth(method="lm", color="red") +
  labs(x = "Selectivity of graduate program index (adjusting for student potential)",
       y = "Quality of first job index (predicted)") +
  theme_bw()
```
:::

:::

---

## Is **doing** *really* that different from **seeing**?

:::{.columns}

:::{.column width="40%"}
Where are we getting the data from?

```{r, cache = TRUE}
# Registry of job market candidates
prob <- 1/(1+exp(-(X-Y*2)))
data$C <- rbinom(N, 1, prob=prob)

lm(Ytrue~Xrand, data=subset(data, C==1))
```
:::


:::{.column width="60%"}
```{r, echo=FALSE, out.width='120%', cache = FALSE}
ggplot2::ggplot(subset(data, C==1), 
       aes(x=Xrand, y=Ytrue)) +
  geom_point(alpha=0.2) +
  geom_smooth(method="lm", color="red") +
  labs(x = "Selectivity of graduate program index",
       y = "Quality of first job index") +
  theme_bw()
```
:::

:::

---
