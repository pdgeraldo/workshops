---
title: "Structural Causal Model"
author: "Pablo Geraldo Bast√≠as"
logo: "https://fundit.fr/sites/default/files/styles/max_650x650/public/institutions/capture-decran-2023-07-07-162216.png?itok=1CkwlJEu"
include-in-header:
  - text: |
      <style>
      .reveal .slide-logo {
        max-height: unset;
        height: 100px;
      }
      </style>
footer: "Demystifying Causal Inference - SCM"
date: 05/28/2024
date-format: long
format: 
  revealjs:
    theme: simple
    width: 1600
    height: 900
    transition: slide
    slide-number: c/t
    chalkboard: true
    auto-stretch: false
callout-appearance: minimal
---

# Outline {background-color="#17a091"}

\newcommand\indep{\perp\!\!\!\perp}
\newcommand\nindep{\not\!\perp\!\!\!\perp}

## Lecture 2: From causal assumptions to causal models

## Recap from previous session

* We studied the difference between **seeing** (the focus of Stats/ML) and **doing** (the focus of causal inference)

* We studied the importance of using the formal language  of potential outcomes to:
  
  + clarify *what do we want to know* (**estimand**)
  
  + identify *reasons for discrepancies* between what we observe and our target (**bias**) 
  
  + formalize *what needs to be true*  for our estimand to be identified with a given *estimator* (**assumptions**)
  
  
* We studied the role of **randomisation** to identify causal effects **by design**.

. . .

But what do we do when we have *less than perfect* experiments?

. . .

And how to assess our assumptions with *observational data*?]

# Graphical models {background-color="#17a091"}

## Why graphical models?

Formally, potential outcomes are sufficient to specify our estimand, sources of bias, and assumptions needed for causal identification.

. . .

However, assessing the plausibility of identification assumptions rely on researchers being able to reason about (conditional) independence between possibly *counterfactual* variables. 

. . .

Any ideas on how to assess the assumption about the *conditional* independence of the **potential outcomes** with respect to the treatment, without randomization?

. . .

We can certainly *understand* the statement saying that the treatment is assigned **as-if random** adjusting for covariates. But what about its plausibility?

---

## Why graphical models?
### Assessing ignorability


When we say the treatment assignment is **strongly ignorable** we are stating that $P(Y_d|D=d) = P(Y_d)$, but we never get to observe the full distribution of potential outcomes!


* What type of criteria should we use when discussing others' causal claims? 

* What kind of criteria should we use in our own research to judge if we are getting what we are looking for?

Here is where DAGs shine, offering a graphical criteria that is equivalent to the unconfoundedness statement, the *backdoor criterion*.


## Structural Causal Model (SCM)

Unifying approach to causal inference, developed by Pearl, Robins, among others, following early developments by Wright:

* (Non-parametric) Structural Equation Models
  
* Generalization of the path analysis and SEM you might be familiar with

* Graphical representation using Directed Acyclic Graphs (DAGs)

* Potential Outcomes are **derived** from a SCM

* Transparent representation of **qualitative assumptions**

* Testable implications of our model of the data generating process
---


## Directed Acyclic Graphs (DAGs)

Probabilistic graphical models are mathematical objects that represent relations among variables (probability factorization) 

They are compounded by two ingredients: **nodes** (vertices) and **edges** (links)

Directed Acyclic Graphs (DAGs) are one class of graphical models, with the following characteristics:

. . .

* **Directed**: The edges point *from* one variable *to* another variable

* **Acyclic**: The paths in the graph flow in certain direction, if you follow the edges you cannot arrive back to the starting point

* **Graph**: well, you get it!

. . .

:::{.callout-warning}
## Important! Under certain conditions, a DAG can be causally interpreted, in which case we talk about "causal DAGs" or causal diagrams

Basically, this happen when we assume that no pair of nodes share a common ancestor that is not included in the DAG
:::


---

## Paths

We can go from one variable to another following a *path* along the edges 

When you can traverse a path without colliding into an edge in the opposite direction we call it a **connecting path** that transmit information

When you encounter an edge pointing into the opposite direction along a path we call it a **blocking path** that do not transmit information


:::{.callout-warning}
## Faithfulness
$d-$connection in the graph implies association between variables in the data, while $d-$separation implies their independence
:::


## Building blocks

* A **chain**, in which you can travel from $X$ to $Y$ through $M$, is a $d-$connected path: $$X \rightarrow M \rightarrow Y$$

* A **fork**, in which you can go from a common cause $W$ to both $X$ and $Y$ is a $d-$connected path: $$X \leftarrow W \rightarrow Y$$

* A **collider**, in which you can't go from $X$ to $Y$ due to two edges pointing into a third variable $C$, is a $d-$separated path: $$X \rightarrow C \leftarrow Y$$


## Adjustment

By adjusting for a variable (represented by a box in the graph), we can turn connecting into blocking paths and vice versa:

* When you adjust for the intermediate variable $M$ in a chain, $X$ and $Y$ become conditionally independent: $$X \rightarrow \boxed{M} \rightarrow Y$$

* When you adjust for the common cause $W$ in a fork, $X$ and $Y$ become conditionally independent: $$X \leftarrow \boxed{W} \rightarrow Y$$

* When you adjust for a collider variable $C$, the pair $X$ and $Y$ become conditionally associated: $$X \cdots \boxed{C} \cdots Y$$


## Confounding




