---
title: "Causal Inference for Social Sciences"
subtitle: "A quick and dirty introduction"
author: "Pablo Geraldo Bast√≠as (pdgeraldo@ucla.edu)"
institute: "SICSS - UCLA"
date: "June 18th, 2020"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      ratio: '16:10'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

$$\newcommand\indep{\perp\!\!\!\perp}$$
$$\newcommand\nindep{\not\!\perp\!\!\!\perp}$$

```{r setup, include=FALSE}
library(tidyverse)
library(ggdag)
```

## Why should we study causal inference?

<br>

The social sciences are experimenting what some authors have called a "credibility revolution" (Angrist and Pischke, 2010), an "identification revolution" (Morgan, 2016), or simply a "causal revolution" (Pearl and MacKenzie, 2018).

In artificial intelligence/ML, causality have been deemed ["the next frontier"](https://phys.org/news/2019-02-causal-disentanglement-frontier-ai.html) and ["the next most important thing"](https://www.datasciencecentral.com/profiles/blogs/causality-the-next-most-important-thing-in-ai-ml).

The enourmous progress in the last decades has been facilitated by the development of a mathematical framework that provide researchers with tools to handle causal questions: Potential Outcomes and Structural Causal Model.

---



## What should you expect from this workshop?

This workshop is designed as a "Crash Course", so we can obviously focus only on a few things:

<br>

* Familiarize yourself with the most used CI frameworks

* Use potential outcomes and the do-operator to formalize causal estimands

* Use directed acyclic graphs (DAGs) to encode qualitative assumptions

* Derive identification results and testable implications from a DAG 

* Assess the plausibility of different identification strategies applied to real problems

<br>

At the end of our session, I hope you feel better equipped to read and evaluate the applied literature and to design your own studies using appropriate identification strategies (or at least having a clear idea on what to look for and where!).

---


## What are we not covering today?

There is a lot of stuff out there! Some areas that you might be interested in but we don't have enough time to review today:

<br>

* Sequential estimation for time-varying treatments (g-methods)

* Estimation in general, including Targeted Maximum Likelihood

* Machine learning for heterogeneous treatment effects

* Do-calculus and PO-calculus for identification

* Causal mediation analysis, causal attribution

* Other graphical models (MCM, SWIGs, the Hypothetical Model)

* Selection diagrams for missing data

* Data fusion (aka generalization, external validity)

* Causal discovery (going from the data to the DAG)

---



## Intuitions about causality
### Have you heard any of these before?

<br>
"Correlation does not imply causation"

--


"No causation without manipulation"

--

"Causal inference is a missing data problem"

--

"Design thriumps analysis"

--

<br>

.center[Any other mantra?]

???
## Intuitions about causality
### Have you heard any of these before?

<br>
"Correlation does not imply causation" $\rightarrow$ But can we go from one to the other?

"No causation without manipulation" $\rightarrow$ What about race or gender?

"Causal inference is a missing data problem" $\rightarrow$ Or the other way around?

"Design thriumps analysis" $\rightarrow$ But what do we mean by design? And analysis?



---
class: center, middle

# What is causal inference about?
---


## Statistics/ML vs Causal Inference
.pull-left[
  ###Statistics/ML

  + Passive observation of the data generating process
  + Estimand: Joint probabilities, CEF
   $$P(X,Y)$$
   $$E(Y|X)$$
  + Focus on asymptotics / out of sample prediction
  + Estimation problem: variance-bias tradeoff
  + Pearl: "deep learning is just curve fitting"
]

.pull-right[
  ###Causal Inference
  
  + Prediction under interventions on the data generating process
  + Estimand: interventional quantities
  $$P(Y|do(x))$$ 
  $$E(Y|do(x)) - E(Y|do(x'))$$ 
  $$= E(Y_x) - E(Y_{x'})$$
  + Identification problem: consistency (infinite sample)
  + Estimation problem: in general, focus on bias over variance (but changing)
]

---



## The ladder of causality 

| Level | Estimand | Activity | Field/Discipline | Example |
|-------| -------- | -------- | ----- | ------- |
| Association | $\mathbf{P(Y \vert X)}$ | Seeing, Observing | Stats, Machine Learning | *What would I believe about Y if I see X?* <br> What is the expected income of a college graduate? |
| Intervention | $\mathbf{P(Y \vert do(x))}$ | Doing, Intervening | Experiments, Policy evaluation | *What would happen with Y if I do X?* <br> What would be my income if I graduate from college? |
| Counterfactual | $\mathbf{P(Y_x \vert x',y')}$ | Imagining, Retrospecting | Structural Models | *What would have happened with Y have I done X instead of X'? Why?* <br> What would have been my parents' income have they graduated from college, given that they didn't attend?| 

.footnote[Pearl and Mackenzie (2018)]

???
In PO, generally the interventional and the counterfactual level are treated as equivalent. But you will never get an unbiased estimation of counterfactual quantities using experiments!

---


## Is **doing** *really* that different from **seeing**?

Let's imagine an example: the effect of the graduate program attended on the quality of your first job after graduation

.pull-left[
```{r}
set.seed(1988)
# sample size
N <- 10000
# student selectivity
W <- rnorm(N, mean=250, sd=50) 
# program selectivity
X <- 0.6*W + rnorm(N, mean=0, sd=10)
# quality of first job
Y <- 0.3*W - 0.2*X + rnorm(N) #<<
data <- tibble(Y=Y, X=X, W=W)
```
]

.pull-right[
```{r, echo=FALSE, out.width='80%'}
ggplot(data, aes(x=X, y=Y)) +
  geom_point(alpha=0.3) +
  geom_smooth(method="lm", color="red") +
  labs(x = "Graduate program quality index",
       y = "First job quality index") +
  scale_x_continuous(limits = c(0,320)) +
  scale_y_continuous(limits = c(-40,120)) +
  theme_bw()
```
]

---


## Is **doing** *really* that different from **seeing**?

Let's imagine an example: the effect of the graduate program attended on the quality of your first job after graduation

.pull-left[
Let's see what a linear regression would tell
```{r}
lm(Y~X, data=data)
```

However, we know that the effect is negative!
]

.pull-right[
```{r, echo=FALSE, out.width='80%'}
ggplot(data, aes(x=X, y=Y)) +
  geom_point(alpha=0.3) +
  geom_smooth(method="lm", color="red") +
  labs(x = "Graduate program quality index",
       y = "First job quality index") +
  scale_x_continuous(limits = c(0,320)) +
  scale_y_continuous(limits = c(-40,120)) +
  theme_bw()
```
]

---



## Is **doing** *really* that different from **seeing**?

.pull-left[
What if we instead randomize students to different programs?
```{r}
# let's randomize students to programs!
X <- sample(min(X):max(X), #<<
            N, replace=TRUE)
# quality of first job
Y <- 0.3*W - 0.2*X + rnorm(N)
data$Xrand <- X
data$Ytrue <- Y

lm(Ytrue~Xrand, data=data)
```
]

.pull-right[
```{r, echo=FALSE, out.width='80%'}
ggplot(data, aes(x=Xrand, y=Ytrue)) +
  geom_point(alpha=0.3) +
  geom_smooth(method="lm", color="red") +
  labs(x = "Graduate program quality index",
       y = "First job quality index") +
  scale_x_continuous(limits = c(0,320)) +
  scale_y_continuous(limits = c(-40,120)) +
  theme_bw()
```
]

---




## Is **doing** *really* that different from **seeing**?

.pull-left[
Can we fix the observational data by adjusting for $W$?
```{r}
# Same regression
# Plus controls
lm(Y~X+W, data=data)
```
]

.pull-right[
```{r, echo=FALSE, out.width='80%'}
ggplot(data, aes(x=X, y=Y)) +
  geom_point(alpha=0.3) +
  geom_smooth(method="lm", color="red") +
  labs(x = "Graduate program quality index",
       y = "First job quality index") +
  scale_x_continuous(limits = c(0,320)) +
  scale_y_continuous(limits = c(-40,120)) +
  theme_bw()
```
]

---

## Is **doing** *really* that different from **seeing**?

.pull-left[
Where are we getting the data from?
```{r}
# Registry of JMC
prob <- 1/(1+exp(-(X-Y*2)))
data$C <- rbinom(N, 1, prob=prob)

lm(Ytrue~Xrand, data=data %>% 
     filter(C==1))
```
]

.pull-right[
```{r, echo=FALSE, out.width='80%'}
ggplot(data %>% filter(C==1), 
       aes(x=Xrand, y=Ytrue)) +
  geom_point(alpha=0.3) +
  geom_smooth(method="lm", color="red") +
  labs(x = "Graduate program quality index",
       y = "First job quality index") +
  scale_x_continuous(limits = c(0,320)) +
  scale_y_continuous(limits = c(-40,120)) +
  theme_bw()
```
]

---



class: center, middle

# Potential Outcomes
---


## Potential Outcomes


Introduced by Neyman (1923) in the context of experimental design. They remained only used in that context for decades!

<br>

* Imported and developed by Donald Rubin for observational studies (1974)

* They are really great to clarify *what do we want to know* (**estimand**)

* This includes identifying *reasons for discrepancies* between what we observe and our estimand (**bias**) 

* They are great to formalize *what needs to be true*  for our estimand to be identified with a given *estimator* (**assumptions**)

* They are not-so-great to assess if our assumptions are plausible or defensible (more on this soon!)

---


## Potential Outcomes: Notation


Let's start with some definitions: 

<br>

$Y$ is the outcome variable *as we observe it*

$X$ is the variable whose effect we want to study (treatment, exposure)

$Y_x$ is the potential outcome when we **set** $X=x$. For example, when $X \in \{0,1\}$:

  * $Y_1$ is the potential outcome under "treatment"

  * $Y_0$ is the potential outcome under "control"


---



## Potential Outcomes: Notation

You will find a lot of equivalent notations for potential outcomes. It could be confusing, but it is good to get practice working with different variants (at least if you want to read the papers!)

$$Y(x) = Y_x = Y^x$$

**Read it like this**: 

The value that the variable $Y$ would take if we set the variable $X$ to the value $x$.

.center[Can you ever observe any of those potential outcomes?]

--

**Consistency** (also known as SUTVA)

$$X = x \rightarrow Y = Y_x$$

For the binary treatment case, we have:

$$Y = XY_1 + (1-X)Y_0$$

---


## Potential Outcomes: The Science Table

```{r, echo=FALSE}
data <- 
  tibble(X=c(rep("A",5),rep("B",5)),
         Ya=c(1,0,0,1,1,1,1,0,0,1),
         Yb=c(1,0,1,0,1,0,0,0,1,1),
         W=c("quant","quant","qual","quant","qual",
             "qual","quant","qual","qual","quant")) %>%
  mutate(Y = ifelse(X=="A",Ya,Yb))

ATE <- mean(data$Ya-data$Yb)
ATE_qual <- mean(data$Ya[data$W=="qual"]) -
  mean(data$Yb[data$W=="qual"])
ATE_quant <- mean(data$Ya[data$W=="quant"]) -
  mean(data$Yb[data$W=="quant"])
DIM <- mean(data$Y[data$X=="A"]) - mean(data$Y[data$X=="B"])
DIM_quant <- mean(data$Y[data$X=="A" & data$W=="quant"]) - 
  mean(data$Y[data$X=="B" & data$W=="quant"])
DIM_qual <- mean(data$Y[data$X=="A" & data$W=="qual"]) - 
  mean(data$Y[data$X=="B" & data$W=="qual"])
weights_quant <- sum(data$W=="quant")/nrow(data)
weights_qual <- sum(data$W=="qual")/nrow(data)
DIM_w <- DIM_quant*weights_quant + DIM_qual*weights_qual
```

One advantage of PO is that we can treat them directly as random variables! So, everything we already know related to probability manipulation still applies here.

.pull-left[
| Unit | $X_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ |
|----|---|---|-----|-------|--------|
| 1 | A | 1 | 1 | 1 | 0 |
| 2 | A | 0 | 0 | 0 | 0 |
| 3 | A | 0 | 0 | 1 |-1 |
| 4 | A | 1 | 1 | 0 | 1 |
| 5 | A | 1 | 1 | 1 | 0 |
| 6 | B | 0 | 1 | 0 | 1 |
| 7 | B | 0 | 1 | 0 | 1 |
| 8 | B | 0 | 0 | 0 | 0 |
| 9 | B | 1 | 0 | 1 |-1 |
| 10 |B | 1 | 1 | 1 | 0 |
]

.pull-right[
The basic calculation device (usually implicit) for that matter is the **science table**. Basically, the full schedule response of the potential outcomes under different treatment conditions
]


---




## Potential Outcomes: Average Treatment Effect

One advantage of PO is that we can treat them directly as random variables! So, everything we already know related to probability manipulation still applies here.

.pull-left[
| Unit | $X_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ |
|----|---|---|-----|-------|--------|
| 1 | A | 1 | 1 | 1 | 0 |
| 2 | A | 0 | 0 | 0 | 0 |
| 3 | A | 0 | 0 | 1 |-1 |
| 4 | A | 1 | 1 | 0 | 1 |
| 5 | A | 1 | 1 | 1 | 0 |
| 6 | B | 0 | 1 | 0 | 1 |
| 7 | B | 0 | 1 | 0 | 1 |
| 8 | B | 0 | 0 | 0 | 0 |
| 9 | B | 1 | 0 | 1 |-1 |
| 10 |B | 1 | 1 | 1 | 0 |
]

.pull-right[
$$\text{ATE} = E(Y_{ai}) - E(Y_{bi})$$
$$(6/10) - (5/10) = \mathbf{\color{blue}{0.1}}$$
]
---


## Potential Outcomes: difference in means

One advantage of PO is that we can treat them directly as random variables! So, everything we already know related to probability manipulation still applies here.

.pull-left[
| Unit | $X_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ |
|----|---|---|-----|-------|--------|
| 1 | A | 1 | 1 | . | . |
| 2 | A | 0 | 0 | . | . |
| 3 | A | 0 | 0 | . | . |
| 4 | A | 1 | 1 | . | . |
| 5 | A | 1 | 1 | . | . |
| 6 | B | 0 | . | 0 | . |
| 7 | B | 0 | . | 0 | . |
| 8 | B | 0 | . | 0 | . |
| 9 | B | 1 | . | 1 | . |
| 10 |B | 1 | . | 1 | . |
]

.pull-right[
$$\text{ATE} = E(Y_{ai}) - E(Y_{bi})$$
$$(6/10) - (5/10) = \mathbf{\color{blue}{0.1}}$$

<br>
$$\text{diff-in-means} = E(Y_i|X=a) - E(Y_i|X=b)$$
$$(3/5)-(2/5) = \mathbf{\color{red}{0.2}}$$
<br>
$$\color{red}{\text{diff-in-means} \neq \text{ATE}}$$

.center[But why?]
]
---


## Potential Outcomes: sources of bias

One advantage of PO is that we can treat them directly as random variables! So, everything we already know related to probability manipulation still applies here.

.pull-left[
| Unit | $X_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ |
|----|---|---|-----|-------|--------|
| 1 | A | 1 | 1 | . | . |
| 2 | A | 0 | 0 | . | . |
| 3 | A | 0 | 0 | . | . |
| 4 | A | 1 | 1 | . | . |
| 5 | A | 1 | 1 | . | . |
| 6 | B | 0 | . | 0 | . |
| 7 | B | 0 | . | 0 | . |
| 8 | B | 0 | . | 0 | . |
| 9 | B | 1 | . | 1 | . |
| 10 |B | 1 | . | 1 | . |
]

.pull-right[
$$E(\text{diff-in-means})$$
$$= E(Y_i|X=a) - E(Y_i|X=b)$$
$$= E(Y_a|X=a) - E(Y_b|X=b)$$
$$= ATE +$$
$$(E[Y_b|X=a] - E[Y_b|X=b])+$$
$$(1-P[X])(ATT-ATC)$$
$$= \mathbf{\color{blue}{0.1}} + \color{red}{0.2 + (0.5)(-0.2) = \mathbf{0.2}}$$
]
---



## Potential Outcomes: identification assumptions

One advantage of PO is that we can treat them directly as random variables! So, everything we already know related to probability manipulation still applies here.

.pull-left[
| Unit | $X_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ |
|----|---|---|-----|-------|--------|
| 1 | A | 1 | 1 | 1 | 0 |
| 2 | A | 0 | 0 | 0 | 0 |
| 3 | A | 0 | 0 | 1 |-1 |
| 4 | A | 1 | 1 | 0 | 1 |
| 5 | A | 1 | 1 | 1 | 0 |
| 6 | B | 0 | 1 | 0 | 1 |
| 7 | B | 0 | 1 | 0 | 1 |
| 8 | B | 0 | 0 | 0 | 0 |
| 9 | B | 1 | 0 | 1 |-1 |
| 10 |B | 1 | 1 | 1 | 0 |
]

.pull-right[
We need the following condition to be true:
$$
Y_x \indep X 
$$

Do we meet that condition here? No!
$$(Y_{ai},Y_{bi}) \nindep X$$

Because: 
$$P(Y_a = y | X=a) \neq P(Y_a = y)$$
$$P(Y_b = y | X=b) \neq P(Y_b = y)$$
]
---


## Potential Outcomes: identification assumptions

One advantage of PO is that we can treat them directly as random variables! So, everything we already know related to probability manipulation still applies here.

.pull-left[
| Unit | $X_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ | $W_i$ |
|----|---|---|-----|-------|--------| --- |
| 1 | A | 1 | 1 | 1 | 0 | Quant |
| 2 | A | 0 | 0 | 0 | 0 | Quant |
| 3 | A | 0 | 0 | 1 |-1 | Qual |
| 4 | A | 1 | 1 | 0 | 1 | Quant |
| 5 | A | 1 | 1 | 1 | 0 | Qual |
| 6 | B | 0 | 1 | 0 | 1 | Qual |
| 7 | B | 0 | 1 | 0 | 1 | Quant |
| 8 | B | 0 | 0 | 0 | 0 | Qual |
| 9 | B | 1 | 0 | 1 |-1 | Qual |
| 10 |B | 1 | 1 | 1 | 0 | Quant |
]

.pull-right[
What about including another covariate $W$?

Does the following condition holds?
$$
Y_x \indep X | W
$$

Not quite either! But still "better" than before, right? Let's define:

$$\text{ATE}_{W} = E(Y_a-Y_b|W=w)$$
and the estimator
$$\widehat{\text{ATE}}_{W} =$$ 
$$E(Y_i|X=a,W=w) - E(Y_i|X=b,W=w)$$
]
---



## Potential Outcomes: identification assumptions

One advantage of PO is that we can treat them directly as random variables! So, everything we already know related to probability manipulation still applies here.

.pull-left[
| Unit | $X_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ | $W_i$ |
|----|---|---|-----|-------|--------| --- |
| 1 | A | 1 | 1 | 1 | 0 | Quant |
| 2 | A | 0 | 0 | 0 | 0 | Quant |
| 3 | A | 0 | 0 | 1 |-1 | Qual |
| 4 | A | 1 | 1 | 0 | 1 | Quant |
| 5 | A | 1 | 1 | 1 | 0 | Qual |
| 6 | B | 0 | 1 | 0 | 1 | Qual |
| 7 | B | 0 | 1 | 0 | 1 | Quant |
| 8 | B | 0 | 0 | 0 | 0 | Qual |
| 9 | B | 1 | 0 | 1 |-1 | Qual |
| 10 |B | 1 | 1 | 1 | 0 | Quant |
]

.pull-right[
$$\widehat{\text{ATE}}_{W=quant} = 0.16$$
$$\widehat{\text{ATE}}_{W=qual} = 0.16$$
$$\widehat{\text{ATE}} = (0.5)(0.16) + (0.5)(0.16) = 0.16$$
However, look a the true $\text{ATE}_W$:
$$\text{ATE}_{W=quant} = -0.2$$
$$\text{ATE}_{W=qual} = 0.4$$
$$\text{ATE} = (0.5)(-0.2) + (0.5)(0.4) = \color{blue}{\mathbf{0.1}}$$
]
---



## Potential Outcomes: how to assess our assumptions?

One advantage of PO is that we can treat them directly as random variables! So, everything we already know related to probability manipulation still applies here.

.pull-left[
| Unit | $X_i$ | $Y_i$ | $Y_{ai}$ | $Y_{bi}$ | $\tau_i$ | $W_i$ |
|----|---|---|-----|-------|--------| ---|
| 1 | A | 1 | 1 | . | . | Quant |
| 2 | A | 0 | 0 | . | . | Quant |
| 3 | A | 0 | 0 | . | . | Qual |
| 4 | A | 1 | 1 | . | . | Quant |
| 5 | A | 1 | 1 | . | . | Qual |
| 6 | B | 0 | . | 0 | . | Qual |
| 7 | B | 0 | . | 0 | . | Quant |
| 8 | B | 0 | . | 0 | . | Qual |
| 9 | B | 1 | . | 1 | . | Qual |
| 10 |B | 1 | . | 1 | . | Quant |
]

.pull-right[
In general, we rely on **extra-statistical assumptions** about the data generating process to claim causal identification.

<br>
>"No causes in, no cases out"
>> Nancy Cartwright

<br>

The question is how to assess our identification strategies, with very limited information, while being transparent about what we are assuming!
]
---

## Short activity: Breakout rooms

Let's denote our treatment variable as $D \in \{0,1\}$, observed outcome $Y$ and potential outcomes $Y_0$ and $Y_1$. We have some measured covariates $X$ and some unmeasured covariates $U$.

Below, I listed a serie of commonly invoked identification assumptions. For each assumption, explain it in plain terms, and  provide an example in which the statement would be true:

* Random assignment, independence of potential outcomes and treatment: 

$$\{Y_0,Y_1\} \indep D$$

* Selection on observables, unconfoundedness, or conditional ignorability:

$$\{Y_0,Y_1\} \indep D | X$$

* Parallel trends, difference-in-difference:

$$E[Y_0(t=1)-Y_0(t=0) | D = 0] = E[Y_0(t=1)-Y_0(t=0) | D = 1]$$



---



## Potential Outcomes: key insights

<br>

* They are really great to clarify *what do we want to know* (**estimand**)

--

* This includes identifying *reasons for discrepancies* between what we observe and our estimand (**bias**) 

--

* They are great to formalize *what needs to be true*  for our estimand to be identified with a given *estimator* (**assumptions**)

--

* They are not-so-great to assess if our assumptions are plausible or defensible

--

<br>

.center[Can anyone tell how to assess the assumption about the *conditional* independence of the **potential outcomes** with respecto to the treatment?]

--

.center[We can certainly *understand* the statement saying that the treatment is assigned **as-if random** adjusting for covariates. But what about its plausibility?]

---




## Potential Outcomes: limitations

<br>

One problem with this formulation is how to assess the plausibility of the conditional independence assumption when it does not hold by design.

We are stating that $P(Y_x|X=x) = P(Y_x)$, but we never get to observe the full distribution of potential outcomes.

What type of criteria should we use when discussing others' causal claims? What kind of criteria should we use in our own research to judge if we are getting what we are looking for?

Here is where DAGs shine, offering a graphical criteria that is equivalent to the unconfoundedness statement, the *backdoor criterion*.


---



class: center, middle

# Structural Causal Model
---


## Structural Causal Model (SCM)

Unifying approach to causal inference, developed by Pearl, Robins, among others:

<br>

* (Non-parametric) Structural Equation Models
  * Generalization of the path analysis and SEM you might be familiar with

* Graphical representation using Directed Acyclic Graphs (DAGs)

* Potential Outcomes are **derived** from a SCM

* Transparent representation of **qualitative assumptions**

* Testable implications of our model of the data generating process
---


## Directed Acyclic Graphs: Notation

Probabilistic graphical models are mathematical objects that represent relations among variables (probability factorizations) 

They are compounded by two ingredients: **nodes** (vertices) and **edges** (links)

Directed Acyclic Graphs (DAGs) are one class of graphical models, with the following characteristics:

* **Directed**: The edges point *from* one variable *to* another variable

* **Acyclic**: The paths in the graph flow in certain direction, if you follow the edges you cannot arrive back to the starting point

* **Graph**: well, you get it!

$\color{red}{\text{Important:}}$: under certain conditions, a DAG can be causally interpreted, in which case we talk about "causal DAGs" or causal diagrams

Basically, this happen when we assume that no pair of nodes share a common ancestor that is not included in the DAG


---

## Directed Acyclic Graphs: Notation


We can go from one variable to another following a *path* along the edges 

When you can traverse a path without colliding into an edge in the opposite direction we call it a **connecting path** that transmit information

When you encounter an edge pointing into the opposite direction along a path we call it a **blocking path** that do not transmit information

<br>

$\color{red}{\text{Faithfulness:}}$ $d-$connection in the graph implies association between variables in reality, while $d-$separation implies their independence

---

## Directed Acyclic Graphs: Notation


We can go from one variable to another following a *path* along the edges

When you can traverse a path without colliding into an edge in the opposite direction we call it a **connecting path** that transmit information

When you encounter an edge pointing into the opposite direction along a path we call it a **blocking path** that do not transmit information

* A chain, in which you can travel from $X$ to $Y$ through $M$, is a $d-$connected path: $$X \rightarrow M \rightarrow Y$$

* A fork, in which you can go from a common cause $W$ to both $X$ and $Y$ is a $d-$connected path: $$X \leftarrow W \rightarrow Y$$

* A collider, in which you can't go from $X$ to $Y$ due to two edges pointing into a third variable $C$, is a $d-$separated path: $$X \rightarrow C \leftarrow Y$$

---


## Directed Acyclic Graphs: Notation


By adjusting for a variable (represented by a box in the graph), we can turn connecting into blocking paths and vice versa

Therefore, we can have *conditional* $d-$separation, and *conditional* $d-$connection

* When you adjust for the intermediate variable $M$ in a chain, $X$ and $Y$ become conditionally independent: $$X \rightarrow \boxed{M} \rightarrow Y$$

* When you adjust for the common cause $W$ in a fork, $X$ and $Y$ become conditionally independent: $$X \leftarrow \boxed{W} \rightarrow Y$$

* When you adjust for a collider variable $C$, the pair $X$ and $Y$ become conditionally associated: $$X \cdots \boxed{C} \cdots Y$$

---



## Confounding paths

.pull-left[
| Confounding |
|-----|
| $W = f_w(U_w)$ |
| $X = f_x(W, U_x)$ |
| $Y = f_y(W, X, U_y)$ |

<br>

Are $X$ and $Y$ marginally independent?

Are they conditionally independent?
]

.pull-right[

.center[<img src="img\confounder1.pdf" width="300" height="200" />]

The DAG includes the following paths

$$X \rightarrow Y$$

$$X \leftarrow W \rightarrow Y$$
]
---

## Confounding paths

.pull-left[
| Confounding |
|-----|
| $W = f_w(U_w)$ |
| $X = f_x(W, U_x)$ |
| $Y = f_y(W, X, U_y)$ |


<br>

Are $X$ and $Y$ marginally independent?

Are they conditionally independent?
]

.pull-right[

.center[<img src="img\confounder3.pdf" width="300" height="200" />]

The following path is open

$$X \rightarrow Y$$

But this is now closed

$$X \leftarrow \boxed{W} \rightarrow Y$$
]

---

## Confounding paths

.pull-left[
| Confounding |
|-----|
| $W = f_w(U_w)$ |
| $X = f_x(W, U_x)$ |
| $Y = f_y(W, U_y)$ |

<br>

Are $X$ and $Y$ marginally independent?

Are they conditionally independent?

Is there a causal effect of $X$ on $Y$?
]

.pull-right[

.center[<img src="img\confounder2.pdf" width="300" height="200" />]

The DAG includes the following path

$$X \leftarrow W \rightarrow Y$$
]
---


## Mediating paths

.pull-left[
| Mediation |
|-----|
| $X = f_x(U_x)$ |
| $M = f_m(X, U_m)$ |
| $Y = f_y(M, X, U_y)$ |

<br>

Are $X$ and $Y$ marginally independent?

Are they conditionally independent?

Is there a causal effect of $X$ on $Y$?
]

.pull-right[

.center[<img src="img/mediation1.pdf" width="300" height="200" />]

The DAG includes the following paths

$$X \rightarrow Y$$

$$X \rightarrow M \rightarrow Y$$
]
---


## Mediating paths

.pull-left[
| Mediation |
|-----|
| $X = f_x(U_x)$ |
| $M = f_m(X, U_m)$ |
| $Y = f_y(M, X, U_y)$ |

<br>

Are $X$ and $Y$ marginally independent?

Are they conditionally independent?

Is there a causal effect of $X$ on $Y$?
]

.pull-right[

.center[<img src="img/mediation2.pdf" width="300" height="200" />]

The DAG includes the following path

$$X \rightarrow Y$$

But this path is now closed

$$X \rightarrow \boxed{M} \rightarrow Y$$
]
---


## Colliding paths

.pull-left[
| Colliders |
|-----|
| $X = f_x(U_x)$ |
| $Y = f_y(W, X, U_y)$ |
| $C = f_c(X, Y, U_c)$ |

<br>

Are $X$ and $Y$ marginally independent?

Are they conditionally independent?

Is there a causal effect of $X$ on $Y$?
]

.pull-right[

.center[<img src="img\collider1.pdf" width="300" height="200" />]

The DAG includes the following paths

$$X \rightarrow Y$$

$$X \rightarrow C \leftarrow Y$$
]
---

## Colliding paths

.pull-left[
| Colliders |
|-----|
| $X = f_x(U_x)$ |
| $Y = f_y(W, U_y)$ |
| $C = f_c(X, Y, U_c)$ |

<br>

Are $X$ and $Y$ marginally independent?

Are they conditionally independent?

Is there a causal effect of $X$ on $Y$?
]

.pull-right[

.center[<img src="img\collider2.pdf" width="300" height="200" />]

The DAG includes the following path

$$X \rightarrow C \leftarrow Y$$
]
---

## Colliding paths

.pull-left[
| Colliders |
|-----|
| $X = f_x(U_x)$ |
| $Y = f_y(W, X, U_y)$ |
| $C = f_c(X, Y, U_c)$ |

<br>

Are $X$ and $Y$ marginally independent?

Are they conditionally independent?

Is there a causal effect of $X$ on $Y$?
]

.pull-right[

.center[<img src="img\collider3.pdf" width="300" height="200" />]

The DAG includes the following (open) path

$$X \rightarrow \boxed{C} \leftarrow Y$$
]
---

## Side note: are colliders that important?

<br> 

One common question (and an area of debate among practitioners) is if colliders are really that important *in applied settings*. This is a hard question to answer, because, you know... we just don't know. 

But we know that they are **possible** and that their importance would depend on the structure of our causal graph.

A few compelling examples of collider bias in recent social sciences are discussed by:

* [Shalizi and Thomas (2011)](https://journals.sagepub.com/doi/abs/10.1177/0049124111404820) in the context of network homophily and contagion

* [Richard Breen (2018)](https://academic.oup.com/esr/article/34/6/603/5094485) in the context of intergenerational mobility

* [Knox, Lowe and Mummolo (2020)](https://scholar.princeton.edu/jmummolo/publications/bias-built-how-administrative-records-mask-racially-biased-policing) in the context of police shootings

A great general introduction to the topic is offered by [Elwert and Winship (2014)](https://www.annualreviews.org/doi/abs/10.1146/annurev-soc-071913-043455)

---


## Do-operator and interventions

<br>

Pearl introduced the $do-$operator to clearly distinguish between passive observations and interventions on the data generating process

In other words, is a form to make explicit the gap between interventional quantities and traditional conditional expectations

Causal identification corresponds to removing the $do-$operator from an expression, following the rules of $do-$calculus, reducing it to an observational quantity. If there is no equivalece, it means that the quantity of interest is not identified

Given the correspondence between a system of non-parametric structural equations and a given DAG, we can express the operation of **doing** as a *minimal surgery on the structural equation defining the treatment*

---

## Interventional graphs

.pull-left[
Let's start with the following observational data generating process

| Structural Causal Model |
|---|
| $Z_1 = f_{z1}(U_{z1})$ |
| $Z_2 = f_{z2}(U_{z2})$ |
| $W = f_{w}(Z_1, Z_2, U_{w})$ |
| $X = f_{z1}(Z_1, W, U_{x})$ |
| $Y = f_{y}(Z_2, W, U_{y})$ |

]

.pull-right[
.center[<img src="img\intervention1.pdf" width="300" height="300" />]
]

---

## Interventional graphs

.pull-left[
Intervening in the model to make $X=x$ creates an interventional graph $G_{\bar{X}}$, in which all the incoming arrows into $X$ have been removed

| Structural Causal Model |
|---|
| $Z_1 = f_{z1}(U_{z1})$ |
| $Z_2 = f_{z2}(U_{z2})$ |
| $W = f_{w}(Z_1, Z_2, U_{w})$ |
| $X = x$ |
| $Y = f_{y}(Z_2, W, U_{y})$ |

]

.pull-right[
.center[<img src="img\intervention2.pdf" width="300" height="300" />]
]

???
Mention invariance of the remaining functions!!!

---



## Interventional graphs

.pull-left[
The purpose of an observational study is to allow only **causal paths** between the treatment $X$ and the outcome $Y$, and block all the **non causal paths**

| Structural Causal Model |
|---|
| $Z_1 = f_{z1}(U_{z1})$ |
| $Z_2 = f_{z2}(U_{z2})$ |
| $W = f_{w}(Z_1, Z_2, U_{w})$ |
| $X = f_{z1}(Z_1, W, U_{x})$ |
| $Y = f_{y}(Z_2, W, U_{y})$ |

]

.pull-right[
.center[<img src="img\intervention3.pdf" width="300" height="300" />]

Adjusting for $W$ blocks a non-causal path, but opens a new one. 

$P(Y|do(x))$ is not identified conditioning on $W$ alone
]

???
This is to relativize the idea of "confounder" as a property of the variables, instead of a property of paths relative to the pair (X,Y)

---



## Interventional graphs

.pull-left[
The purpose of an observational study is to allow only **causal paths** between the treatment $X$ and the outcome $Y$, and block all the **non causal paths**

| Structural Causal Model |
|---|
| $Z_1 = f_{z1}(U_{z1})$ |
| $Z_2 = f_{z2}(U_{z2})$ |
| $W = f_{w}(Z_1, Z_2, U_{w})$ |
| $X = f_{z1}(Z_1, W, U_{x})$  |
| $Y = f_{y}(Z_2, W, U_{y})$ |

]

.pull-right[
.center[<img src="img\intervention4.pdf" width="300" height="300" />]

Adjusting for $(Z_1,W)$ blocks all non-causal paths

$P(Y|do(x))$ is identified conditioning on $(Z_1,W)$
]

---



## Interventional graphs

.pull-left[
The purpose of an observational study is to allow only **causal paths** between the treatment $X$ and the outcome $Y$, and block all the **non causal paths**

| Structural Causal Model |
|---|
| $Z_1 = f_{z1}(U_{z1})$ |
| $Z_2 = f_{z2}(U_{z2})$ |
| $W = f_{w}(Z_1, Z_2, U_{w})$ |
| $X = f_{z1}(Z_1, W, U_{x})$  |
| $Y = f_{y}(Z_2, W, U_{y})$ |

]

.pull-right[
.center[<img src="img\intervention5.pdf" width="300" height="300" />]

Adjusting for $(Z_2,W)$ also blocks all non-causal paths

$P(Y|do(x))$ is identified conditioning on $(Z_2,W)$
]

---

## Back-door Criterion (Pearl)

<br>

What we just did can be summarized by the back-door criterion

A set of variables $W$ satisfied the back-door criterion relative to an ordered pair of variables $(X,Y)$ in a DAG $G$ if:

  (i) no node in $W$ is a descendant of $X$; and
  (ii) $W$ blocks every path between $X$ and $Y$ that contains an arrow into $X$

$\color{red}{\text{Important:}}$ satifying the backdoor criterion implies the unconfoundedness assumption
$$Y_x \indep X|W$$

The **back-door adjustment** indicates that we can recover the effect of $X$ on $Y$ adjusting for any $W$ that satisfy the backdoor criterion

$$P(Y|do(x)) = \sum_w P(Y|X,W)P(W)$$

.footnote[Pearl, *Causality*, pp.79-81]

---

## Short activity: Breakout rooms

Open [www.dagitty.net](http://www.dagitty.net) and launch it. There, you can create a DAG, obtain a list of testable implications, and check if an effect is identified under the DAG.

* Skim the following paper by [Sharkey et al.](https://www.rootcausecoalition.org/wp-content/uploads/2018/08/Community-and-the-Crime-Decline-The-Causal-Effect-of-Local-Nonprofits-on-Violent-Crime.pdf) (see Analytical Approach) and try to reconstruct the underlying DAG for the long-term model:

  * Share your result
  * Is the effect identified?
  * Can you think in possible violations of their assumptions?
  
* Skim the following paper by [Doyle et al.](https://www.rootcausecoalition.org/wp-content/uploads/2018/08/Community-and-the-Crime-Decline-The-Causal-Effect-of-Local-Nonprofits-on-Violent-Crime.pdf) (see Section III. Empirical Strategy) and try to reconstruct the underlying DAG: 

  * Share your result
  * Is the effect identified?
  * Can you think in possible violations of their assumptions?



---



## SCM: Key insights

<br>

* Causal identification is contingent on a given model encoding our assumptions

* Causal identification is finding an observational quantity that is equivalent to an interventional quantity

* Confounding (and bias) is a property of paths in a graph, not variables

* Confounding is relative to the pair $(X,Y)$, not just $X$

* It is not necessary to adjust for all *parents* of the treatment to block all backdoor paths

* Bias is not monotonically decreasing on the number of variables included

* $do-$calculus can be used to identify the effect of multiple interventions, to recover from missingness data, and to generalize study results.

---


##SCM: limitations

<br> 

One problem with the Structural Causal Model framework is that you need to assume a certain DAG. Conditional on your model, most identification tasks are rather trivial (algorithmic)

>>"Causality is in the model"
>>> James Heckman (2005)

However, this is something that we always do! The only matter is how transparent are we about the assumptions we are making anyway

Another problem, more important for issues like mediation analysis and counterfactuals in general, is that we are sometimes making more assumptions that we are willing to

For this cases, other causal (and graphical) models, like the [Single World Intervention Graphs](https://www.csss.washington.edu/Papers/wp128.pdf) (Richardson and Robins, 2013) can help

Being fully non-parametric, certain canonical models are not identified using DAGs (like IVs). However, this only shows that they require parametric assumptions, no matter how weak

---


class: center, middle

# Identification Strategies
---

## Identification Strategies: a short tour

Identification strategies are well-known set of assumptions that are sufficient for the causal interpretation of certain estimators:

* **Randomization** ensures that the potential outcomes are independent from the treatment in experiments. This justifies a causal interpretation of a diff in means estimator.

* **Selection on Observables** (aka Conditional Ignorability, Unconfoundedness) for multiple regression, (propensity score) matching and (inverse probability) weighting.

  * In FE confounders are assumed to be fully captured by constant characteristics of the individual, group, or time.
  
* **Parallel trends** for the difference-in-difference estimator (equivalent to a two-way fixed effects in the two group, two periods case)

  * Generalizations for multiple groups, multiple adoption periods, and synthetic controls
  
* **Instrumental Variables** and quasi-experiments (exogenous variation in the treatment assignment plus exclusion restriction) for the 2SLS and Wald estimator

  * RDD can be interpreted as IVs or under continuity assumptions.
  
---


## Is there a ladder of identification strategies?

<br>
In general, it is assumed that the stronger the assumptions, the less credible an identification strategy would be

We would prefere experiments (the so-called *gold standard*) and, in the lack of experiments, we would prefer strategies in which our assumptions hold **by design**

Designs in which there is an **exogenous** source of variation in the treatment status (like IV, RDD, quasi-experiments in general) are considered more plausible

However, is there a natural hierarchy of identification strategies that can tell us, *a priori*, which assumptions are more credible in empirical applications?

Do empirical applications corresponds to the labels that we use to describe a given research design? Any thoughts?


---

## Debates in applied research

*Partly as a result of the focus on empirical examples the econometrics literature has developed a small number of canonical settings where researchers view the specific causal models and associated statistical methods as well-established and understood. These causal models correspond to what is nowadays often referred to as* identification strategies
>Guido Imbens (2019)

*About 20 years ago, when asked in a meeting what can be done in observational studies to clarify the step from association to causation, Sir Ronald Fisher replied: 'Make your theories elaborate.' The reply puzzled me at first, since by Occam's razor, the advice usually given is to make theories as simple as is consistent with known data. What Sir Ronald meant, as subsequent discussion showed, was that when constructing a causal hypothesis one should envisage as many different consequences of its truth as possible, and plan observational studies to discover whether each of these is found to hold.*
>B. G. Cochran (cited in Rosenbaum, 1995)


*No one should ever write down a 100 variable DAG and do inference based on that. That would be an insane approach because the analysis would be totally impenetrable. Develop a research design where that 100 variable DAG trivially reduces to a familiar problem (e.g. IV!)*
>Jason Abaluck (cited in Imbens, 2019)

---

## Where to go now?

* Structural Causal Model
  * The Book of Why (Pearl and MacKenzie)
  * Causal Inference in Statistics. A Primer (Pearl, Glymour and Jewell)
  * Causality (Pearl)
  
* Potential Outcomes
  * Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction (Imbens and Rubin)
  * Mostly Harmless Econometrics (Angrist and Pischke)
  
* General and integrative introductions
  * Causal Inference: The Mixtape (Cunnigham)
  * Causal Inference (Hernan and Robins)
  * Counterfactuals and Causal Inference (Morgan and Winship)


---

class: center, middle

# We are done! 

---

