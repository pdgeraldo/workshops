
@misc{Benkeserdrtmle2020,
  title = {Drtmle: {{Doubly}}-{{Robust Nonparametric Estimation}} and {{Inference}}},
  shorttitle = {Drtmle},
  author = {Benkeser, David and Hejazi, Nima},
  year = {2020},
  month = jan,
  abstract = {Targeted minimum loss-based estimators of counterfactual means and causal effects that are doubly-robust with respect both to consistency and asymptotic normality (Benkeser et al (2017), {$<$}doi:10.1093/biomet/asx053{$>$}; MJ van der Laan (2014), {$<$}doi:10.1515/ijb-2012-0038{$>$}).},
  copyright = {MIT + file LICENSE}
}

@misc{CutlerrandomForest2018,
  title = {{{randomForest}}: {{Breiman}} and {{Cutler}}'s {{Random Forests}} for {{Classification}} and {{Regression}}},
  shorttitle = {{{randomForest}}},
  author = {Cutler, Fortran original by Leo Breiman {and} Adele and Wiener, R. port by Andy Liaw {and} Matthew},
  year = {2018},
  month = mar,
  abstract = {Classification and regression based on a forest of trees using random inputs, based on Breiman (2001) {$<$}doi:10.1023/A:1010933404324{$>$}.},
  copyright = {GPL-2 | GPL-3 [expanded from: GPL ({$\geq$} 2)]},
  keywords = {Environmetrics,MachineLearning,MissingData}
}

@article{GlynnIntroduction2010,
  title = {An {{Introduction}} to the {{Augmented Inverse Propensity Weighted Estimator}}},
  author = {Glynn, Adam N. and Quinn, Kevin M.},
  year = {2010/ed},
  volume = {18},
  pages = {36--56},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mpp036},
  abstract = {In this paper, we discuss an estimator for average treatment effects (ATEs) known as the augmented inverse propensity weighted (AIPW) estimator. This estimator has attractive theoretical properties and only requires practitioners to do two things they are already comfortable with: (1) specify a binary regression model for the propensity score, and (2) specify a regression model for the outcome variable. Perhaps the most interesting property of this estimator is its so-called ``double robustness.'' Put simply, the estimator remains consistent for the ATE if either the propensity score model or the outcome regression is misspecified but the other is properly specified. After explaining the AIPW estimator, we conduct a Monte Carlo experiment that compares the finite sample performance of the AIPW estimator to three common competitors: a regression estimator, an inverse propensity weighted (IPW) estimator, and a propensity score matching estimator. The Monte Carlo results show that the AIPW estimator has comparable or lower mean square error than the competing estimators when the propensity score and outcome models are both properly specified and, when one of the models is misspecified, the AIPW estimator is superior.},
  file = {/Users/pdgeraldo/Zotero/storage/ZPNZ5HBE/Glynn and Quinn - 2010 - An Introduction to the Augmented Inverse Propensit.pdf},
  journal = {Political Analysis},
  language = {en},
  number = {1}
}

@misc{GreiferWeightIt2020,
  title = {{{WeightIt}}: {{Weighting}} for {{Covariate Balance}} in {{Observational Studies}}},
  shorttitle = {{{WeightIt}}},
  author = {Greifer, Noah},
  year = {2020},
  month = feb,
  abstract = {Generates weights to form equivalent groups in observational studies with point or longitudinal treatments by easing and extending the functionality of the R packages 'twang' for generalized boosted modeling (McCaffrey, Ridgeway \& Morral, 2004) {$<$}doi:10.1037/1082-989X.9.4.403{$>$}, 'CBPS' for covariate balancing propensity score weighting (Imai \& Ratkovic, 2014) {$<$}doi:10.1111/rssb.12027{$>$}, 'ebal' for entropy balancing (Hainmueller, 2012) {$<$}doi:10.1093/pan/mpr025{$>$}, 'optweight' for optimization-based weights (Zubizarreta, 2015) {$<$}doi:10.1080/01621459.2015.1023805{$>$}, 'ATE' for empirical balancing calibration weighting (Chan, Yam, \& Zhang, 2016) {$<$}doi:10.1111/rssb.12129{$>$}, and 'SuperLearner' for stacked machine learning-based propensity scores (Pirracchio, Petersen, \& van der Laan, 2015) {$<$}doi:10.1093/aje/kwu253{$>$}. Also allows for assessment of weights and checking of covariate balance by interfacing directly with 'cobalt'.},
  copyright = {GPL-2 | GPL-3 [expanded from: GPL ({$\geq$} 2)]}
}

@article{HainmuellerEntropy2012a,
  title = {Entropy {{Balancing}} for {{Causal Effects}}: {{A Multivariate Reweighting Method}} to {{Produce Balanced Samples}} in {{Observational Studies}}},
  shorttitle = {Entropy {{Balancing}} for {{Causal Effects}}},
  author = {Hainmueller, Jens},
  year = {2012},
  volume = {20},
  pages = {25--46},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mpr025},
  abstract = {This paper proposes entropy balancing, a data preprocessing method to achieve covariate balance in observational studies with binary treatments. Entropy balancing relies on a maximum entropy reweighting scheme that calibrates unit weights so that the reweighted treatment and control group satisfy a potentially large set of prespecified balance conditions that incorporate information about known sample moments. Entropy balancing thereby exactly adjusts inequalities in representation with respect to the first, second, and possibly higher moments of the covariate distributions. These balance improvements can reduce model dependence for the subsequent estimation of treatment effects. The method assures that balance improves on all covariate moments included in the reweighting. It also obviates the need for continual balance checking and iterative searching over propensity score models that may stochastically balance the covariate moments. We demonstrate the use of entropy balancing with Monte Carlo simulations and empirical applications.},
  file = {/Users/pdgeraldo/Zotero/storage/9HDACWNA/Hainmueller - 2012 - Entropy Balancing for Causal Effects A Multivaria.pdf},
  journal = {Political Analysis},
  language = {en},
  number = {1}
}

@article{HainmuellerKernel2014,
  title = {Kernel {{Regularized Least Squares}}: {{Reducing Misspecification Bias}} with a {{Flexible}} and {{Interpretable Machine Learning Approach}}},
  shorttitle = {Kernel {{Regularized Least Squares}}},
  author = {Hainmueller, Jens and Hazlett, Chad},
  year = {2014},
  volume = {22},
  pages = {143--168},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/mpt019},
  abstract = {We propose the use of Kernel Regularized Least Squares (KRLS) for social science modeling and inference problems. KRLS borrows from machine learning methods designed to solve regression and classification problems without relying on linearity or additivity assumptions. The method constructs a flexible hypothesis space that uses kernels as radial basis functions and finds the best-fitting surface in this space by minimizing a complexity-penalized least squares problem. We argue that the method is well-suited for social science inquiry because it avoids strong parametric assumptions, yet allows interpretation in ways analogous to generalized linear models while also permitting more complex interpretation to examine nonlinearities, interactions, and heterogeneous effects. We also extend the method in several directions to make it more effective for social inquiry, by (1) deriving estimators for the pointwise marginal effects and their variances, (2) establishing unbiasedness, consistency, and asymptotic normality of the KRLS estimator under fairly general conditions, (3) proposing a simple automated rule for choosing the kernel bandwidth, and (4) providing companion software. We illustrate the use of the method through simulations and empirical examples.},
  file = {/Users/pdgeraldo/Zotero/storage/9JS6TPFI/Hainmueller and Hazlett - 2014 - Kernel Regularized Least Squares Reducing Misspec.pdf},
  journal = {Political Analysis},
  language = {en},
  number = {2}
}

@misc{Hansenoptmatch2019,
  title = {Optmatch: {{Functions}} for {{Optimal Matching}}},
  shorttitle = {Optmatch},
  author = {Hansen, Ben B. and Fredrickson, Mark and Buckner, Josh and Errickson, Josh and Solenberger, Adam Rauh {and} Peter and Tseng, with embedded Fortran code due to Dimitri P. Bertsekas {and} Paul},
  year = {2019},
  month = dec,
  abstract = {Distance based bipartite matching using the RELAX-IV minimum cost flow solver, oriented to matching of treatment and control groups in observational studies. Routines are provided to generate distances from generalised linear models (propensity score matching), formulas giving variables on which to limit matched distances, stratified or exact matching directives, or calipers, alone or in combination.},
  copyright = {file LICENSE},
  keywords = {Optimization,SocialSciences}
}

@article{HazlettKernel2016,
  title = {Kernel {{Balancing}}: {{A Flexible Non}}-{{Parametric Weighting Procedure}} for {{Estimating Causal Effects}}},
  shorttitle = {Kernel {{Balancing}}},
  author = {Hazlett, Chad},
  year = {2016},
  issn = {1556-5068},
  doi = {10.2139/ssrn.2746753},
  abstract = {Matching and weighting methods are widely used to estimate causal effects when adjusting for a set of observables is required. Matching is appealing for its non-parametric nature, but with continuous variables, is not guaranteed to remove bias. Weighting techniques choose weights on units to ensure pre-specified functions of the covariates have equal (weighted) means for the treated and control group. This assures unbiased effect estimation only when the potential outcomes are linear in those pre-specified functions of the observables. Kernel balancing begins by assuming the expectation of the non-treatment potential outcome conditional on the covariates falls in a large, flexible space of functions associated with a kernel. It then constructs linear bases for this function space and achieves approximate balance on these bases. A worst-case bound on the bias due to this approximation is given and is the target of minimization. Relative to current practice, kernel balancing offers one reasoned solution to the long-standing question of which functions of the covariates investigators should attempt to achieve (and check) balance on. Further, these weights are also those that would make the estimated multivariate density of covariates approximately the same for the treated and control groups, when the same choice of kernel is used to estimate those densities. The approach is fully automated up to the choice of a kernel and smoothing parameter, for which default options and guidelines are provided. An R package, KBAL, implements this approach.},
  file = {/Users/pdgeraldo/Zotero/storage/6WX78IVV/Hazlett - 2016 - Kernel Balancing A Flexible Non-Parametric Weight.pdf},
  journal = {SSRN Electronic Journal},
  language = {en}
}

@misc{HazlettUCLAKRLS2017,
  title = {{{KRLS}}: {{Kernel}}-{{Based Regularized Least Squares}}},
  shorttitle = {{{KRLS}}},
  author = {Hazlett (UCLA), Jens Hainmueller (Stanford) Chad},
  year = {2017},
  month = jul,
  abstract = {Package implements Kernel-based Regularized Least Squares (KRLS), a machine learning method to fit multidimensional functions y=f(x) for regression and classification problems without relying on linearity or additivity assumptions. KRLS finds the best fitting function by minimizing the squared loss of a Tikhonov regularization problem, using Gaussian kernels as radial basis functions. For further details see Hainmueller and Hazlett (2014).},
  copyright = {GPL-2 | GPL-3 [expanded from: GPL ({$\geq$} 2)]}
}

@article{ImaiCovariate2014,
  title = {Covariate Balancing Propensity Score},
  author = {Imai, Kosuke and Ratkovic, Marc},
  year = {2014},
  volume = {76},
  pages = {243--263},
  issn = {1467-9868},
  doi = {10.1111/rssb.12027},
  abstract = {The propensity score plays a central role in a variety of causal inference settings. In particular, matching and weighting methods based on the estimated propensity score have become increasingly common in the analysis of observational data. Despite their popularity and theoretical appeal, the main practical difficulty of these methods is that the propensity score must be estimated. Researchers have found that slight misspecification of the propensity score model can result in substantial bias of estimated treatment effects. We introduce covariate balancing propensity score (CBPS) methodology, which models treatment assignment while optimizing the covariate balance. The CBPS exploits the dual characteristics of the propensity score as a covariate balancing score and the conditional probability of treatment assignment. The estimation of the CBPS is done within the generalized method-of-moments or empirical likelihood framework. We find that the CBPS dramatically improves the poor empirical performance of propensity score matching and weighting methods reported in the literature. We also show that the CBPS can be extended to other important settings, including the estimation of the generalized propensity score for non-binary treatments and the generalization of experimental estimates to a target population. Open source software is available for implementing the methods proposed.},
  copyright = {\textcopyright{} 2013 Royal Statistical Society},
  file = {/Users/pdgeraldo/Zotero/storage/C4PEUAFJ/Imai and Ratkovic - 2014 - Covariate balancing propensity score.pdf},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  keywords = {Causal inference,Instrumental variables,Inverse propensity score weighting,Marginal structural models,Observational studies,Propensity score matching,Randomized experiments},
  language = {en},
  number = {1}
}

@article{LalondeEvaluating1986,
  title = {Evaluating the {{Econometric Evaluations}} of {{Training Programs}} with {{Experimental Data}}},
  author = {Lalonde, Robert J},
  year = {1986},
  volume = {76},
  pages = {604--620},
  file = {/Users/pdgeraldo/Zotero/storage/VCL2CTRV/Lalonde - Evaluating the Econometric Evaluations of Training.pdf},
  journal = {The American Economic Review},
  language = {en},
  number = {4}
}

@misc{MaoPSW2018,
  title = {{{PSW}}: {{Propensity Score Weighting Methods}} for {{Dichotomous Treatments}}},
  shorttitle = {{{PSW}}},
  author = {Mao, Huzhang and Li, Liang},
  year = {2018},
  month = jan,
  abstract = {Provides propensity score weighting methods to control for confounding in causal inference with dichotomous treatments and continuous/binary outcomes. It includes the following functional modules: (1) visualization of the propensity score distribution in both treatment groups with mirror histogram, (2) covariate balance diagnosis, (3) propensity score model specification test, (4) weighted estimation of treatment effect, and (5) augmented estimation of treatment effect with outcome regression. The weighting methods include the inverse probability weight (IPW) for estimating the average treatment effect (ATE), the IPW for average treatment effect of the treated (ATT), the IPW for the average treatment effect of the controls (ATC), the matching weight (MW), the overlap weight (OVERLAP), and the trapezoidal weight (TRAPEZOIDAL). Sandwich variance estimation is provided to adjust for the sampling variability of the estimated propensity score. These methods are discussed by Hirano et al (2003) {$<$}doi:10.1111/1468-0262.00442{$>$}, Lunceford and Davidian (2004) {$<$}doi:10.1002/sim.1903{$>$}, Li and Greene (2013) {$<$}doi:10.1515/ijb-2012-0030{$>$}, and Li et al (2016) {$<$}doi:10.1080/01621459.2016.1260466{$>$}.},
  copyright = {GPL-2 | GPL-3 [expanded from: GPL ({$\geq$} 2)]}
}

@article{McCaffreyPropensity2004,
  title = {Propensity Score Estimation with Boosted Regression for Evaluating Causal Effects in Observational Studies},
  author = {McCaffrey, Daniel F. and Ridgeway, Greg and Morral, Andrew R.},
  year = {2004},
  month = dec,
  volume = {9},
  pages = {403--425},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.9.4.403},
  abstract = {Causal effect modeling with naturalistic rather than experimental data is challenging. In observational studies participants in different treatment conditions may also differ on pretreatment characteristics that influence outcomes. Propensity score methods can theoretically eliminate these confounds for all observed covariates, but accurate estimation of propensity scores is impeded by large numbers of covariates, uncertain functional forms for their associations with treatment selection, and other problems. This article demonstrates that boosting, a modern statistical technique, can overcome many of these obstacles. The authors illustrate this approach with a study of adolescent probationers in substance abuse treatment programs. Propensity score weights estimated using boosting eliminate most pretreatment group differences and substantially alter the apparent relative effects of adolescent substance abuse treatment.},
  file = {/Users/pdgeraldo/Zotero/storage/MLMHAWHM/McCaffrey et al. - 2004 - Propensity score estimation with boosted regressio.pdf},
  journal = {Psychological Methods},
  keywords = {Evaluation Studies as Topic,Humans,Models; Psychological,Outcome Assessment; Health Care,Psychometrics,Substance-Related Disorders},
  language = {eng},
  number = {4},
  pmid = {15598095}
}


