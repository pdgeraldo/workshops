---
title: "Causal Inference Workshop"
subtitle: "(Day 2: Graphical models)"
author: "Pablo Geraldo (pdgeraldo@ucla.edu)"
institute: "SICSS - UCLA"
date: "June 25, 2021"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    lib_dir: libs
    nature:
      ratio: '16:10'
      highlightStyle: tomorrow
      highlightLines: true
      countIncrementalSlides: false
---

$$\newcommand\indep{\perp\!\!\!\perp}$$
$$\newcommand\nindep{\not\!\perp\!\!\!\perp}$$
```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo(primary_color = "#2d68c4", 
          secondary_color = "#f2a900",
          header_font_google = google_font("Josefin Sans"),
          text_font_google   = google_font("Montserrat", "300", "300i"),
          code_font_google   = google_font("Fira Mono"))
```

```{r setup, include=FALSE}
library(tidyverse)
library(ggdag)
```




## Potential Outcomes 
### Beyond randomization

<br>

* They are really great to clarify *what do we want to know* (**estimand**)

--

* This includes identifying *reasons for discrepancies* between what we observe and our estimand (**bias**) 

--

* They are great to formalize *what needs to be true*  for our estimand to be identified with a given *estimator* (**assumptions**)

--

* They are not-so-great to assess if our assumptions are plausible or defensible

--

<br>

.center[Can anyone tell how to assess the assumption about the *conditional* independence of the **potential outcomes** with respecto to the treatment?]

--

.center[We can certainly *understand* the statement saying that the treatment is assigned **as-if random** adjusting for covariates. But what about its plausibility?]

---




## Potential Outcomes: limitations

<br>

One problem with this formulation is how to assess the plausibility of the conditional independence assumption when it does not hold by design.

We are stating that $P(Y_x|X=x) = P(Y_x)$, but we never get to observe the full distribution of potential outcomes.

What type of criteria should we use when discussing others' causal claims? What kind of criteria should we use in our own research to judge if we are getting what we are looking for?

Here is where DAGs shine, offering a graphical criteria that is equivalent to the unconfoundedness statement, the *backdoor criterion*.

---

class: center, middle

# Structural Causal Model
---


## Structural Causal Model (SCM)

Unifying approach to causal inference, developed by Pearl, Robins, among others:

<br>

* (Non-parametric) Structural Equation Models
  * Generalization of the path analysis and SEM you might be familiar with

* Graphical representation using Directed Acyclic Graphs (DAGs)

* Potential Outcomes are **derived** from a SCM

* Transparent representation of **qualitative assumptions**

* Testable implications of our model of the data generating process
---


## Directed Acyclic Graphs: Notation

Probabilistic graphical models are mathematical objects that represent relations among variables (probability factorizations) 

They are compounded by two ingredients: **nodes** (vertices) and **edges** (links)

Directed Acyclic Graphs (DAGs) are one class of graphical models, with the following characteristics:

* **Directed**: The edges point *from* one variable *to* another variable

* **Acyclic**: The paths in the graph flow in certain direction, if you follow the edges you cannot arrive back to the starting point

* **Graph**: well, you get it!

$\color{red}{\text{Important:}}$: under certain conditions, a DAG can be causally interpreted, in which case we talk about "causal DAGs" or causal diagrams

Basically, this happen when we assume that no pair of nodes share a common ancestor that is not included in the DAG


---

## Directed Acyclic Graphs: Notation


We can go from one variable to another following a *path* along the edges 

When you can traverse a path without colliding into an edge in the opposite direction we call it a **connecting path** that transmit information

When you encounter an edge pointing into the opposite direction along a path we call it a **blocking path** that do not transmit information

<br>

$\color{red}{\text{Faithfulness:}}$ $d-$connection in the graph implies association between variables in reality, while $d-$separation implies their independence

---

## Directed Acyclic Graphs: Notation


We can go from one variable to another following a *path* along the edges

When you can traverse a path without colliding into an edge in the opposite direction we call it a **connecting path** that transmit information

When you encounter an edge pointing into the opposite direction along a path we call it a **blocking path** that do not transmit information

* A chain, in which you can travel from $X$ to $Y$ through $M$, is a $d-$connected path: $$X \rightarrow M \rightarrow Y$$

* A fork, in which you can go from a common cause $W$ to both $X$ and $Y$ is a $d-$connected path: $$X \leftarrow W \rightarrow Y$$

* A collider, in which you can't go from $X$ to $Y$ due to two edges pointing into a third variable $C$, is a $d-$separated path: $$X \rightarrow C \leftarrow Y$$

---


## Directed Acyclic Graphs: Notation


By adjusting for a variable (represented by a box in the graph), we can turn connecting into blocking paths and vice versa

Therefore, we can have *conditional* $d-$separation, and *conditional* $d-$connection

* When you adjust for the intermediate variable $M$ in a chain, $X$ and $Y$ become conditionally independent: $$X \rightarrow \boxed{M} \rightarrow Y$$

* When you adjust for the common cause $W$ in a fork, $X$ and $Y$ become conditionally independent: $$X \leftarrow \boxed{W} \rightarrow Y$$

* When you adjust for a collider variable $C$, the pair $X$ and $Y$ become conditionally associated: $$X \cdots \boxed{C} \cdots Y$$

---



## Confounding paths

.pull-left[
| Confounding |
|-----|
| $W = f_w(U_w)$ |
| $X = f_x(W, U_x)$ |
| $Y = f_y(W, X, U_y)$ |

<br>

Are $X$ and $Y$ marginally independent?

Are they conditionally independent?
]

.pull-right[

.center[<img src="img\confounder1.pdf" width="300" height="200" />]

The DAG includes the following paths

$$X \rightarrow Y$$

$$X \leftarrow W \rightarrow Y$$
]
---

## Confounding paths

.pull-left[
| Confounding |
|-----|
| $W = f_w(U_w)$ |
| $X = f_x(W, U_x)$ |
| $Y = f_y(W, X, U_y)$ |


<br>

Are $X$ and $Y$ marginally independent?

Are they conditionally independent?
]

.pull-right[

.center[<img src="img\confounder3.pdf" width="300" height="200" />]

The following path is open

$$X \rightarrow Y$$

But this is now closed

$$X \leftarrow \boxed{W} \rightarrow Y$$
]

---

## Confounding paths

.pull-left[
| Confounding |
|-----|
| $W = f_w(U_w)$ |
| $X = f_x(W, U_x)$ |
| $Y = f_y(W, U_y)$ |

<br>

Are $X$ and $Y$ marginally independent?

Are they conditionally independent?

Is there a causal effect of $X$ on $Y$?
]

.pull-right[

.center[<img src="img\confounder2.pdf" width="300" height="200" />]

The DAG includes the following path

$$X \leftarrow W \rightarrow Y$$
]
---


## Mediating paths

.pull-left[
| Mediation |
|-----|
| $X = f_x(U_x)$ |
| $M = f_m(X, U_m)$ |
| $Y = f_y(M, X, U_y)$ |

<br>

Are $X$ and $Y$ marginally independent?

Are they conditionally independent?

Is there a causal effect of $X$ on $Y$?
]

.pull-right[

.center[<img src="img/mediation1.pdf" width="300" height="200" />]

The DAG includes the following paths

$$X \rightarrow Y$$

$$X \rightarrow M \rightarrow Y$$
]
---


## Mediating paths

.pull-left[
| Mediation |
|-----|
| $X = f_x(U_x)$ |
| $M = f_m(X, U_m)$ |
| $Y = f_y(M, X, U_y)$ |

<br>

Are $X$ and $Y$ marginally independent?

Are they conditionally independent?

Is there a causal effect of $X$ on $Y$?
]

.pull-right[

.center[<img src="img/mediation2.pdf" width="300" height="200" />]

The DAG includes the following path

$$X \rightarrow Y$$

But this path is now closed

$$X \rightarrow \boxed{M} \rightarrow Y$$
]
---


## Colliding paths

.pull-left[
| Colliders |
|-----|
| $X = f_x(U_x)$ |
| $Y = f_y(W, X, U_y)$ |
| $C = f_c(X, Y, U_c)$ |

<br>

Are $X$ and $Y$ marginally independent?

Are they conditionally independent?

Is there a causal effect of $X$ on $Y$?
]

.pull-right[

.center[<img src="img\collider1.pdf" width="300" height="200" />]

The DAG includes the following paths

$$X \rightarrow Y$$

$$X \rightarrow C \leftarrow Y$$
]
---

## Colliding paths

.pull-left[
| Colliders |
|-----|
| $X = f_x(U_x)$ |
| $Y = f_y(W, U_y)$ |
| $C = f_c(X, Y, U_c)$ |

<br>

Are $X$ and $Y$ marginally independent?

Are they conditionally independent?

Is there a causal effect of $X$ on $Y$?
]

.pull-right[

.center[<img src="img\collider2.pdf" width="300" height="200" />]

The DAG includes the following path

$$X \rightarrow C \leftarrow Y$$
]
---

## Colliding paths

.pull-left[
| Colliders |
|-----|
| $X = f_x(U_x)$ |
| $Y = f_y(W, X, U_y)$ |
| $C = f_c(X, Y, U_c)$ |

<br>

Are $X$ and $Y$ marginally independent?

Are they conditionally independent?

Is there a causal effect of $X$ on $Y$?
]

.pull-right[

.center[<img src="img\collider3.pdf" width="300" height="200" />]

The DAG includes the following (open) path

$$X \rightarrow \boxed{C} \leftarrow Y$$
]
---

## Side note: are colliders that important?

<br> 

One common question (and an area of debate among practitioners) is if colliders are really that important *in applied settings*. This is a hard question to answer, because, you know... we just don't know. 

But we know that they are **possible** and that their importance would depend on the structure of our causal graph.

A few compelling examples of collider bias in recent social sciences are discussed by:

* [Shalizi and Thomas (2011)](https://journals.sagepub.com/doi/abs/10.1177/0049124111404820) in the context of network homophily and contagion

* [Richard Breen (2018)](https://academic.oup.com/esr/article/34/6/603/5094485) in the context of intergenerational mobility

* [Knox, Lowe and Mummolo (2020)](https://scholar.princeton.edu/jmummolo/publications/bias-built-how-administrative-records-mask-racially-biased-policing) in the context of police shootings

A great general introduction to the topic is offered by [Elwert and Winship (2014)](https://www.annualreviews.org/doi/abs/10.1146/annurev-soc-071913-043455)

---


## Do-operator and interventions

<br>

Pearl introduced the $do-$operator to clearly distinguish between passive observations and interventions on the data generating process

In other words, is a form to make explicit the gap between interventional quantities and traditional conditional expectations

Causal identification corresponds to removing the $do-$operator from an expression, following the rules of $do-$calculus, reducing it to an observational quantity. If there is no equivalece, it means that the quantity of interest is not identified

Given the correspondence between a system of non-parametric structural equations and a given DAG, we can express the operation of **doing** as a *minimal surgery on the structural equation defining the treatment*

---

## Interventional graphs

.pull-left[
Let's start with the following observational data generating process

| Structural Causal Model |
|---|
| $Z_1 = f_{z1}(U_{z1})$ |
| $Z_2 = f_{z2}(U_{z2})$ |
| $W = f_{w}(Z_1, Z_2, U_{w})$ |
| $X = f_{z1}(Z_1, W, U_{x})$ |
| $Y = f_{y}(Z_2, W, U_{y})$ |

]

.pull-right[
.center[<img src="img\intervention1.pdf" width="300" height="300" />]
]

---

## Interventional graphs

.pull-left[
Intervening in the model to make $X=x$ creates an interventional graph $G_{\bar{X}}$, in which all the incoming arrows into $X$ have been removed

| Structural Causal Model |
|---|
| $Z_1 = f_{z1}(U_{z1})$ |
| $Z_2 = f_{z2}(U_{z2})$ |
| $W = f_{w}(Z_1, Z_2, U_{w})$ |
| $X = x$ |
| $Y = f_{y}(Z_2, W, U_{y})$ |

]

.pull-right[
.center[<img src="img\intervention2.pdf" width="300" height="300" />]
]

???
Mention invariance of the remaining functions!!!

---



## Interventional graphs

.pull-left[
The purpose of an observational study is to allow only **causal paths** between the treatment $X$ and the outcome $Y$, and block all the **non causal paths**

| Structural Causal Model |
|---|
| $Z_1 = f_{z1}(U_{z1})$ |
| $Z_2 = f_{z2}(U_{z2})$ |
| $W = f_{w}(Z_1, Z_2, U_{w})$ |
| $X = f_{z1}(Z_1, W, U_{x})$ |
| $Y = f_{y}(Z_2, W, U_{y})$ |

]

.pull-right[
.center[<img src="img\intervention3.pdf" width="300" height="300" />]

Adjusting for $W$ blocks a non-causal path, but opens a new one. 

$P(Y|do(x))$ is not identified conditioning on $W$ alone
]

???
This is to relativize the idea of "confounder" as a property of the variables, instead of a property of paths relative to the pair (X,Y)

---



## Interventional graphs

.pull-left[
The purpose of an observational study is to allow only **causal paths** between the treatment $X$ and the outcome $Y$, and block all the **non causal paths**

| Structural Causal Model |
|---|
| $Z_1 = f_{z1}(U_{z1})$ |
| $Z_2 = f_{z2}(U_{z2})$ |
| $W = f_{w}(Z_1, Z_2, U_{w})$ |
| $X = f_{z1}(Z_1, W, U_{x})$  |
| $Y = f_{y}(Z_2, W, U_{y})$ |

]

.pull-right[
.center[<img src="img\intervention4.pdf" width="300" height="300" />]

Adjusting for $(Z_1,W)$ blocks all non-causal paths

$P(Y|do(x))$ is identified conditioning on $(Z_1,W)$
]

---



## Interventional graphs

.pull-left[
The purpose of an observational study is to allow only **causal paths** between the treatment $X$ and the outcome $Y$, and block all the **non causal paths**

| Structural Causal Model |
|---|
| $Z_1 = f_{z1}(U_{z1})$ |
| $Z_2 = f_{z2}(U_{z2})$ |
| $W = f_{w}(Z_1, Z_2, U_{w})$ |
| $X = f_{z1}(Z_1, W, U_{x})$  |
| $Y = f_{y}(Z_2, W, U_{y})$ |

]

.pull-right[
.center[<img src="img\intervention5.pdf" width="300" height="300" />]

Adjusting for $(Z_2,W)$ also blocks all non-causal paths

$P(Y|do(x))$ is identified conditioning on $(Z_2,W)$
]

---

## Back-door Criterion (Pearl)

<br>

What we just did can be summarized by the back-door criterion

A set of variables $W$ satisfied the back-door criterion relative to an ordered pair of variables $(X,Y)$ in a DAG $G$ if:

  (i) no node in $W$ is a descendant of $X$; and
  (ii) $W$ blocks every path between $X$ and $Y$ that contains an arrow into $X$

$\color{red}{\text{Important:}}$ satifying the backdoor criterion implies the unconfoundedness assumption
$$Y_x \indep X|W$$

The **back-door adjustment** indicates that we can recover the effect of $X$ on $Y$ adjusting for any $W$ that satisfy the backdoor criterion

$$P(Y|do(x)) = \sum_w P(Y|X,W)P(W)$$

.footnote[Pearl, *Causality*, pp.79-81]

---

## Short activity: Breakout rooms

Open [www.dagitty.net](http://www.dagitty.net) and launch it. There, you can create a DAG, obtain a list of testable implications, and check if an effect is identified under the DAG.

* Skim the following paper by [Sharkey et al.](https://www.rootcausecoalition.org/wp-content/uploads/2018/08/Community-and-the-Crime-Decline-The-Causal-Effect-of-Local-Nonprofits-on-Violent-Crime.pdf) (see Analytical Approach) and try to reconstruct the underlying DAG for the long-term model:

  * Share your result
  * Is the effect identified?
  * Can you think in possible violations of their assumptions?
  
* Skim the following paper by [Doyle et al.](https://www.rootcausecoalition.org/wp-content/uploads/2018/08/Community-and-the-Crime-Decline-The-Causal-Effect-of-Local-Nonprofits-on-Violent-Crime.pdf) (see Section III. Empirical Strategy) and try to reconstruct the underlying DAG: 

  * Share your result
  * Is the effect identified?
  * Can you think in possible violations of their assumptions?



---



## SCM: Key insights

<br>

* Causal identification is contingent on a given model encoding our assumptions

* Causal identification is finding an observational quantity that is equivalent to an interventional quantity

* Confounding (and bias) is a property of paths in a graph, not variables

* Confounding is relative to the pair $(X,Y)$, not just $X$

* It is not necessary to adjust for all *parents* of the treatment to block all backdoor paths

* Bias is not monotonically decreasing on the number of variables included

* $do-$calculus can be used to identify the effect of multiple interventions, to recover from missingness data, and to generalize study results.

---


##SCM: limitations

<br> 

One problem with the Structural Causal Model framework is that you need to assume a certain DAG. Conditional on your model, most identification tasks are rather trivial (algorithmic)

>>"Causality is in the model"
>>> James Heckman (2005)

However, this is something that we always do! The only matter is how transparent are we about the assumptions we are making anyway

Another problem, more important for issues like mediation analysis and counterfactuals in general, is that we are sometimes making more assumptions that we are willing to

For this cases, other causal (and graphical) models, like the [Single World Intervention Graphs](https://www.csss.washington.edu/Papers/wp128.pdf) (Richardson and Robins, 2013) can help

Being fully non-parametric, certain canonical models are not identified using DAGs (like IVs). However, this only shows that they require parametric assumptions, no matter how weak

---


class: center, middle

# Identification Strategies
---

## Identification Strategies: a short tour

Identification strategies are well-known set of assumptions that are sufficient for the causal interpretation of certain estimators:

* **Randomization** ensures that the potential outcomes are independent from the treatment in experiments. This justifies a causal interpretation of a diff in means estimator.

* **Selection on Observables** (aka Conditional Ignorability, Unconfoundedness) for multiple regression, (propensity score) matching and (inverse probability) weighting.

  * In FE confounders are assumed to be fully captured by constant characteristics of the individual, group, or time.
  
* **Parallel trends** for the difference-in-difference estimator (equivalent to a two-way fixed effects in the two group, two periods case)

  * Generalizations for multiple groups, multiple adoption periods, and synthetic controls
  
* **Instrumental Variables** and quasi-experiments (exogenous variation in the treatment assignment plus exclusion restriction) for the 2SLS and Wald estimator

  * RDD can be interpreted as IVs or under continuity assumptions.
  
---


## Is there a ladder of identification strategies?

<br>
In general, it is assumed that the stronger the assumptions, the less credible an identification strategy would be

We would prefere experiments (the so-called *gold standard*) and, in the lack of experiments, we would prefer strategies in which our assumptions hold **by design**

Designs in which there is an **exogenous** source of variation in the treatment status (like IV, RDD, quasi-experiments in general) are considered more plausible

However, is there a natural hierarchy of identification strategies that can tell us, *a priori*, which assumptions are more credible in empirical applications?

Do empirical applications corresponds to the labels that we use to describe a given research design? Any thoughts?


---

## Debates in applied research

*Partly as a result of the focus on empirical examples the econometrics literature has developed a small number of canonical settings where researchers view the specific causal models and associated statistical methods as well-established and understood. These causal models correspond to what is nowadays often referred to as* identification strategies
>Guido Imbens (2019)

*About 20 years ago, when asked in a meeting what can be done in observational studies to clarify the step from association to causation, Sir Ronald Fisher replied: 'Make your theories elaborate.' The reply puzzled me at first, since by Occam's razor, the advice usually given is to make theories as simple as is consistent with known data. What Sir Ronald meant, as subsequent discussion showed, was that when constructing a causal hypothesis one should envisage as many different consequences of its truth as possible, and plan observational studies to discover whether each of these is found to hold.*
>B. G. Cochran (cited in Rosenbaum, 1995)


*No one should ever write down a 100 variable DAG and do inference based on that. That would be an insane approach because the analysis would be totally impenetrable. Develop a research design where that 100 variable DAG trivially reduces to a familiar problem (e.g. IV!)*
>Jason Abaluck (cited in Imbens, 2019)

---

## Where to go now?

* Structural Causal Model
  * The Book of Why (Pearl and MacKenzie)
  * Causal Inference in Statistics. A Primer (Pearl, Glymour and Jewell)
  * Causality (Pearl)
  
* Potential Outcomes
  * Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction (Imbens and Rubin)
  * Mostly Harmless Econometrics (Angrist and Pischke)
  
* General and integrative introductions
  * Causal Inference: The Mixtape (Cunnigham)
  * Causal Inference (Hernan and Robins)
  * Counterfactuals and Causal Inference (Morgan and Winship)


---

class: center, middle

# We are done! 

---
